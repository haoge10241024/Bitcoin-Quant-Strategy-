#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºæ¨¡å‹ç³»ç»Ÿ - è§£å†³ç¼ºå¤±å€¼å¤„ç†å’Œæ¨¡å‹æ€§èƒ½é—®é¢˜
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer, KNNImputer
import matplotlib.pyplot as plt
import seaborn as sns

class AdvancedDataPreprocessor:
    """é«˜çº§æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.scalers = {}
        self.imputers = {}
        
    def advanced_missing_value_handling(self, factors):
        """é«˜çº§ç¼ºå¤±å€¼å¤„ç†"""
        print("ğŸ”§ æ‰§è¡Œé«˜çº§ç¼ºå¤±å€¼å¤„ç†...")
        
        # åªé€‰æ‹©æ•°å€¼åˆ—
        numeric_factors = factors.select_dtypes(include=[np.number])
        processed_factors = numeric_factors.copy()
        
        if len(processed_factors.columns) == 0:
            print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—")
            return factors
        
        # æŒ‰å› å­ç±»å‹åˆ†åˆ«å¤„ç†
        factor_types = self._classify_factors(processed_factors)
        
        for factor_type, factor_list in factor_types.items():
            if len(factor_list) > 0:
                if factor_type == 'price':
                    # ä»·æ ¼å› å­ç”¨å‰å‘å¡«å……
                    processed_factors[factor_list] = processed_factors[factor_list].fillna(method='ffill')
                    processed_factors[factor_list] = processed_factors[factor_list].fillna(method='bfill')
                    
                elif factor_type == 'volume':
                    # æˆäº¤é‡å› å­ç”¨0å¡«å……
                    processed_factors[factor_list] = processed_factors[factor_list].fillna(0)
                    
                elif factor_type == 'technical':
                    # æŠ€æœ¯æŒ‡æ ‡ç”¨KNNå¡«å……
                    if len(factor_list) > 1:
                        try:
                            imputer = KNNImputer(n_neighbors=5)
                            processed_factors[factor_list] = imputer.fit_transform(processed_factors[factor_list])
                        except:
                            # å¦‚æœKNNå¤±è´¥ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……
                            processed_factors[factor_list] = processed_factors[factor_list].fillna(
                                processed_factors[factor_list].median()
                            )
                    else:
                        processed_factors[factor_list] = processed_factors[factor_list].fillna(
                            processed_factors[factor_list].median()
                        )
                
                else:
                    # å…¶ä»–å› å­ç”¨ä¸­ä½æ•°å¡«å……
                    processed_factors[factor_list] = processed_factors[factor_list].fillna(
                        processed_factors[factor_list].median()
                    )
        
        print(f"âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼Œå¤„ç†å‰ç¼ºå¤±å€¼: {numeric_factors.isnull().sum().sum()}, å¤„ç†å: {processed_factors.isnull().sum().sum()}")
        
        return processed_factors
    
    def _classify_factors(self, factors):
        """å› å­åˆ†ç±»"""
        factor_types = {
            'price': [],
            'volume': [],
            'technical': [],
            'statistical': [],
            'other': []
        }
        
        for col in factors.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in ['price', 'ma_', 'ema_', 'return']):
                factor_types['price'].append(col)
            elif any(keyword in col_lower for keyword in ['volume', 'vol_']):
                factor_types['volume'].append(col)
            elif any(keyword in col_lower for keyword in ['rsi', 'macd', 'bb_', 'kdj', 'williams']):
                factor_types['technical'].append(col)
            elif any(keyword in col_lower for keyword in ['volatility', 'skew', 'kurt', 'sharpe']):
                factor_types['statistical'].append(col)
            else:
                factor_types['other'].append(col)
        
        return factor_types
    
    def robust_scaling(self, factors):
        """ç¨³å¥æ ‡å‡†åŒ–"""
        print("ğŸ“ æ‰§è¡Œç¨³å¥æ ‡å‡†åŒ–...")
        
        scaler = RobustScaler()
        scaled_factors = pd.DataFrame(
            scaler.fit_transform(factors),
            index=factors.index,
            columns=factors.columns
        )
        
        self.scalers['robust'] = scaler
        
        print("âœ… æ ‡å‡†åŒ–å®Œæˆ")
        return scaled_factors


class EnhancedFeatureEngineering:
    """å¢å¼ºç‰¹å¾å·¥ç¨‹"""
    
    def __init__(self):
        pass
    
    def create_advanced_features(self, data):
        """åˆ›å»ºé«˜çº§ç‰¹å¾"""
        print("ğŸ”¬ åˆ›å»ºé«˜çº§ç‰¹å¾...")
        
        enhanced_data = data.copy()
        
        # 1. æŠ€æœ¯æŒ‡æ ‡å¢å¼º
        enhanced_data = self._add_advanced_technical_indicators(enhanced_data)
        
        # 2. äº¤äº’ç‰¹å¾
        enhanced_data = self._add_interaction_features(enhanced_data)
        
        # 3. æ—¶é—´ç‰¹å¾
        enhanced_data = self._add_time_features(enhanced_data)
        
        # 4. ç»Ÿè®¡ç‰¹å¾
        enhanced_data = self._add_statistical_features(enhanced_data)
        
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæ–°å¢ç‰¹å¾æ•°é‡: {len(enhanced_data.columns) - len(data.columns)}")
        
        return enhanced_data
    
    def _add_advanced_technical_indicators(self, data):
        """æ·»åŠ é«˜çº§æŠ€æœ¯æŒ‡æ ‡"""
        # å¸ƒæ—å¸¦ä½ç½®
        required_bb_cols = ['bb_upper_20', 'bb_lower_20', 'close']
        if all(col in data.columns for col in required_bb_cols):
            data['bollinger_position'] = (data['close'] - data['bb_lower_20']) / (data['bb_upper_20'] - data['bb_lower_20'])
        
        # RSIèƒŒç¦»
        if 'rsi_14' in data.columns:
            data['rsi_divergence'] = data['rsi_14'].diff()
            data['rsi_momentum'] = data['rsi_14'].rolling(5).mean() - data['rsi_14'].rolling(20).mean()
        
        # MACDå¢å¼º
        if 'macd' in data.columns:
            data['macd_momentum'] = data['macd'].diff()
            data['macd_acceleration'] = data['macd_momentum'].diff()
        
        # æˆäº¤é‡ä»·æ ¼è¶‹åŠ¿
        if all(col in data.columns for col in ['volume', 'close']):
            data['volume_price_trend'] = data['volume'] * data['close'].pct_change()
            volume_ma = data['volume'].rolling(20).mean()
            data['volume_ma_ratio'] = data['volume'] / volume_ma.replace(0, np.nan)
        
        return data
    
    def _add_interaction_features(self, data):
        """æ·»åŠ äº¤äº’ç‰¹å¾"""
        # ç§»åŠ¨å¹³å‡äº¤å‰
        if all(col in data.columns for col in ['ma_5', 'ma_20']):
            data['ma_cross_5_20'] = (data['ma_5'] > data['ma_20']).astype(int)
            data['ma_distance_5_20'] = (data['ma_5'] - data['ma_20']) / data['ma_20'].replace(0, np.nan)
        
        # æ³¢åŠ¨ç‡åˆ¶åº¦
        if 'volatility_20' in data.columns:
            try:
                data['volatility_regime'] = pd.qcut(
                    data['volatility_20'].rank(method='first'), 
                    3, 
                    labels=[0, 1, 2],
                    duplicates='drop'
                ).astype(float)
            except:
                # å¦‚æœåˆ†ä½æ•°åˆ‡åˆ†å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„ä¸‰åˆ†æ³•
                vol_33 = data['volatility_20'].quantile(0.33)
                vol_67 = data['volatility_20'].quantile(0.67)
                data['volatility_regime'] = np.where(
                    data['volatility_20'] <= vol_33, 0,
                    np.where(data['volatility_20'] <= vol_67, 1, 2)
                )
        
        # ä»·æ ¼åŠ¨é‡ç»„åˆ
        if all(col in data.columns for col in ['return_5', 'return_20']):
            data['momentum_combination'] = data['return_5'] * data['return_20']
        
        return data
    
    def _add_time_features(self, data):
        """æ·»åŠ æ—¶é—´ç‰¹å¾"""
        if 'datetime' in data.columns:
            data['hour'] = pd.to_datetime(data['datetime']).dt.hour
            data['day_of_week'] = pd.to_datetime(data['datetime']).dt.dayofweek
            data['month'] = pd.to_datetime(data['datetime']).dt.month
        
        return data
    
    def _add_statistical_features(self, data):
        """æ·»åŠ ç»Ÿè®¡ç‰¹å¾"""
        # æ”¶ç›Šç‡çš„é«˜é˜¶çŸ©
        if 'return_1' in data.columns:
            data['return_skew_20'] = data['return_1'].rolling(20).skew()
            data['return_kurt_20'] = data['return_1'].rolling(20).kurt()
        
        # ä»·æ ¼åˆ†ä½æ•°ä½ç½®
        if 'close' in data.columns:
            data['price_percentile_60'] = data['close'].rolling(60).rank(pct=True)
        
        return data


class EnsembleModelSystem:
    """é›†æˆæ¨¡å‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.models = {}
        self.weights = {}
        self.performance_history = {}
        
    def initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        self.models = {
            'rf': RandomForestRegressor(
                n_estimators=200,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42
            ),
            'gb': GradientBoostingRegressor(
                n_estimators=150,
                learning_rate=0.1,
                max_depth=6,
                subsample=0.8,
                random_state=42
            ),
            'ridge': Ridge(alpha=10.0),
            'lasso': Lasso(alpha=0.1)
        }
    
    def train_ensemble(self, X, y):
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        print("ğŸ¤– è®­ç»ƒé›†æˆæ¨¡å‹...")
        
        # ç¡®ä¿æ•°æ®æ¸…æ´
        X_clean = X.fillna(X.median())
        y_clean = y.fillna(y.median())
        
        # è®­ç»ƒå„ä¸ªæ¨¡å‹
        model_scores = {}
        
        for name, model in self.models.items():
            try:
                # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
                tscv = TimeSeriesSplit(n_splits=5)
                scores = []
                
                for train_idx, val_idx in tscv.split(X_clean):
                    X_train, X_val = X_clean.iloc[train_idx], X_clean.iloc[val_idx]
                    y_train, y_val = y_clean.iloc[train_idx], y_clean.iloc[val_idx]
                    
                    model.fit(X_train, y_train)
                    pred = model.predict(X_val)
                    score = r2_score(y_val, pred)
                    scores.append(score)
                
                model_scores[name] = np.mean(scores)
                print(f"   {name}: RÂ² = {model_scores[name]:.4f}")
                
            except Exception as e:
                print(f"   è­¦å‘Š: {name} è®­ç»ƒå¤±è´¥: {e}")
                model_scores[name] = -1.0
        
        # è®¡ç®—æƒé‡ï¼ˆåŸºäºæ€§èƒ½ï¼‰
        self._calculate_ensemble_weights(model_scores)
        
        # åœ¨å…¨éƒ¨æ•°æ®ä¸Šé‡æ–°è®­ç»ƒ
        for name, model in self.models.items():
            try:
                model.fit(X_clean, y_clean)
            except Exception as e:
                print(f"   è­¦å‘Š: {name} æœ€ç»ˆè®­ç»ƒå¤±è´¥: {e}")
        
        print("âœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        return model_scores
    
    def _calculate_ensemble_weights(self, model_scores):
        """è®¡ç®—é›†æˆæƒé‡"""
        # å°†è´Ÿåˆ†æ•°è®¾ä¸º0
        positive_scores = {name: max(0, score) for name, score in model_scores.items()}
        
        # å½’ä¸€åŒ–æƒé‡
        total_score = sum(positive_scores.values())
        
        if total_score > 0:
            self.weights = {name: score / total_score for name, score in positive_scores.items()}
        else:
            # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½è¡¨ç°ä¸å¥½ï¼Œä½¿ç”¨å‡ç­‰æƒé‡
            self.weights = {name: 1.0 / len(self.models) for name in self.models.keys()}
        
        print(f"   é›†æˆæƒé‡: {self.weights}")
    
    def predict_ensemble(self, X):
        """é›†æˆé¢„æµ‹"""
        X_clean = X.fillna(X.median())
        
        predictions = {}
        for name, model in self.models.items():
            try:
                predictions[name] = model.predict(X_clean)
            except Exception as e:
                print(f"   è­¦å‘Š: {name} é¢„æµ‹å¤±è´¥: {e}")
                predictions[name] = np.zeros(len(X_clean))
        
        # åŠ æƒå¹³å‡
        ensemble_pred = np.zeros(len(X_clean))
        for name, pred in predictions.items():
            ensemble_pred += self.weights.get(name, 0) * pred
        
        return ensemble_pred


class ImprovedTargetDefinition:
    """æ”¹è¿›çš„ç›®æ ‡å˜é‡å®šä¹‰"""
    
    def __init__(self):
        pass
    
    def create_multiple_targets(self, data):
        """åˆ›å»ºå¤šä¸ªç›®æ ‡å˜é‡"""
        print("ğŸ¯ åˆ›å»ºæ”¹è¿›çš„ç›®æ ‡å˜é‡...")
        
        targets = {}
        
        # æŒ‰åˆ†ç»„å¤„ç†
        for (symbol, timeframe), group in data.groupby(['symbol', 'timeframe']):
            group = group.reset_index(drop=True).copy()
            
            # 1. å¤šæœŸæ”¶ç›Šç‡
            group['target_1d'] = group['close'].pct_change().shift(-1)
            group['target_3d'] = group['close'].pct_change(3).shift(-3)
            group['target_5d'] = group['close'].pct_change(5).shift(-5)
            
            # 2. é£é™©è°ƒæ•´æ”¶ç›Š
            volatility = group['close'].pct_change().rolling(20).std()
            group['risk_adjusted_return_1d'] = group['target_1d'] / (volatility + 1e-6)
            
            # 3. æ–¹å‘æ€§ç›®æ ‡
            group['direction_1d'] = (group['target_1d'] > 0).astype(int)
            
            # 4. åˆ†ä½æ•°ç›®æ ‡
            group['return_quantile'] = pd.qcut(
                group['target_1d'].rank(method='first'), 
                5, 
                labels=[0, 1, 2, 3, 4]
            ).astype(float)
            
            targets[(symbol, timeframe)] = group
        
        combined_data = pd.concat(targets.values(), ignore_index=True)
        
        print("âœ… ç›®æ ‡å˜é‡åˆ›å»ºå®Œæˆ")
        return combined_data


class EnhancedModelSystem:
    """å¢å¼ºæ¨¡å‹ç³»ç»Ÿ"""
    
    def __init__(self):
        self.preprocessor = AdvancedDataPreprocessor()
        self.feature_engineer = EnhancedFeatureEngineering()
        self.ensemble_system = EnsembleModelSystem()
        self.target_creator = ImprovedTargetDefinition()
        
        self.results = {}
        
    def run_enhanced_modeling(self, data_path, factor_path=None):
        """è¿è¡Œå¢å¼ºå»ºæ¨¡"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºæ¨¡å‹ç³»ç»Ÿ...")
        
        # 1. åŠ è½½æ•°æ®
        print("\n" + "="*50)
        print("ç¬¬1æ­¥: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")
        print("="*50)
        
        data = pd.read_csv(data_path)
        if 'datetime' in data.columns:
            data['datetime'] = pd.to_datetime(data['datetime'])
        
        if factor_path:
            factors = pd.read_csv(factor_path)
        else:
            factors = self._create_basic_factors(data)
        
        # 2. é«˜çº§æ•°æ®é¢„å¤„ç†
        factors_processed = self.preprocessor.advanced_missing_value_handling(factors)
        factors_scaled = self.preprocessor.robust_scaling(factors_processed)
        
        # 3. ç‰¹å¾å·¥ç¨‹
        print("\n" + "="*50)
        print("ç¬¬2æ­¥: å¢å¼ºç‰¹å¾å·¥ç¨‹")
        print("="*50)
        
        # åˆå¹¶æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹ - é‡ç½®ç´¢å¼•é¿å…é‡å¤
        data_reset = data.reset_index(drop=True)
        factors_reset = factors_processed.reset_index(drop=True)
        
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        min_length = min(len(data_reset), len(factors_reset))
        data_truncated = data_reset.iloc[:min_length]
        factors_truncated = factors_reset.iloc[:min_length]
        
        combined_data = pd.concat([data_truncated, factors_truncated], axis=1)
        enhanced_data = self.feature_engineer.create_advanced_features(combined_data)
        
        # 4. ç›®æ ‡å˜é‡åˆ›å»º
        print("\n" + "="*50)
        print("ç¬¬3æ­¥: æ”¹è¿›ç›®æ ‡å˜é‡å®šä¹‰")
        print("="*50)
        
        data_with_targets = self.target_creator.create_multiple_targets(enhanced_data)
        
        # 5. å‡†å¤‡å»ºæ¨¡æ•°æ®
        feature_columns = [col for col in data_with_targets.columns 
                          if col not in ['timestamp', 'datetime', 'symbol', 'timeframe', 
                                       'open', 'high', 'low', 'close', 'volume'] 
                          and not col.startswith('target_') 
                          and not col.startswith('direction_')
                          and not col.startswith('risk_adjusted_')
                          and not col.startswith('return_quantile')]
        
        X = data_with_targets[feature_columns].select_dtypes(include=[np.number])
        y = data_with_targets['target_1d'].dropna()
        
        # ç¡®ä¿ç´¢å¼•å¯¹é½
        common_index = X.index.intersection(y.index)
        X = X.loc[common_index]
        y = y.loc[common_index]
        
        # 6. é›†æˆæ¨¡å‹è®­ç»ƒ
        print("\n" + "="*50)
        print("ç¬¬4æ­¥: é›†æˆæ¨¡å‹è®­ç»ƒ")
        print("="*50)
        
        self.ensemble_system.initialize_models()
        model_scores = self.ensemble_system.train_ensemble(X, y)
        
        # 7. æ¨¡å‹è¯„ä¼°
        print("\n" + "="*50)
        print("ç¬¬5æ­¥: æ¨¡å‹æ€§èƒ½è¯„ä¼°")
        print("="*50)
        
        evaluation_results = self._evaluate_models(X, y)
        
        # 8. ä¿å­˜ç»“æœ
        self.results = {
            'model_scores': model_scores,
            'evaluation_results': evaluation_results,
            'feature_count': len(feature_columns),
            'data_shape': X.shape,
            'ensemble_weights': self.ensemble_system.weights
        }
        
        self._generate_enhanced_report()
        
        print("\nğŸ‰ å¢å¼ºå»ºæ¨¡å®Œæˆï¼")
        return self.results
    
    def _create_basic_factors(self, data):
        """åˆ›å»ºåŸºç¡€å› å­"""
        print("   è®¡ç®—åŸºç¡€å› å­...")
        
        factors_list = []
        
        for (symbol, timeframe), group in data.groupby(['symbol', 'timeframe']):
            group = group.reset_index(drop=True).copy()
            
            # åŸºç¡€å› å­
            for period in [5, 10, 20, 60]:
                group[f'return_{period}'] = group['close'].pct_change(period)
                group[f'ma_{period}'] = group['close'].rolling(period).mean()
                group[f'volatility_{period}'] = group['close'].pct_change().rolling(period).std()
            
            # æŠ€æœ¯æŒ‡æ ‡
            group['rsi_14'] = self._calculate_rsi(group['close'], 14)
            group['macd'] = group['close'].ewm(span=12).mean() - group['close'].ewm(span=26).mean()
            
            # å¸ƒæ—å¸¦
            ma20 = group['close'].rolling(20).mean()
            std20 = group['close'].rolling(20).std()
            group['bb_upper_20'] = ma20 + 2 * std20
            group['bb_lower_20'] = ma20 - 2 * std20
            
            factors_list.append(group)
        
        all_factors = pd.concat(factors_list, ignore_index=True)
        
        factor_columns = [col for col in all_factors.columns 
                         if col not in ['timestamp', 'datetime', 'symbol', 'timeframe', 
                                      'open', 'high', 'low', 'close', 'volume']]
        
        return all_factors[factor_columns]
    
    def _calculate_rsi(self, prices, period=14):
        """è®¡ç®—RSI"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0).rolling(period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _evaluate_models(self, X, y):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # æ—¶é—´åºåˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=5)
        
        results = {}
        
        # è¯„ä¼°é›†æˆæ¨¡å‹
        ensemble_scores = []
        ensemble_direction_acc = []
        
        for train_idx, test_idx in tscv.split(X):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # è®­ç»ƒé›†æˆæ¨¡å‹
            self.ensemble_system.train_ensemble(X_train, y_train)
            
            # é¢„æµ‹
            pred = self.ensemble_system.predict_ensemble(X_test)
            
            # è¯„ä¼°
            r2 = r2_score(y_test, pred)
            direction_acc = np.mean(np.sign(y_test) == np.sign(pred))
            
            ensemble_scores.append(r2)
            ensemble_direction_acc.append(direction_acc)
        
        results['ensemble'] = {
            'r2_mean': np.mean(ensemble_scores),
            'r2_std': np.std(ensemble_scores),
            'direction_accuracy_mean': np.mean(ensemble_direction_acc),
            'direction_accuracy_std': np.std(ensemble_direction_acc)
        }
        
        print(f"   é›†æˆæ¨¡å‹ RÂ²: {results['ensemble']['r2_mean']:.4f} Â± {results['ensemble']['r2_std']:.4f}")
        print(f"   æ–¹å‘å‡†ç¡®ç‡: {results['ensemble']['direction_accuracy_mean']:.4f} Â± {results['ensemble']['direction_accuracy_std']:.4f}")
        
        return results
    
    def _generate_enhanced_report(self):
        """ç”Ÿæˆå¢å¼ºæŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆå¢å¼ºæ¨¡å‹æŠ¥å‘Š...")
        
        report = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'system_improvements': {
                'advanced_preprocessing': True,
                'enhanced_feature_engineering': True,
                'ensemble_modeling': True,
                'improved_target_definition': True
            },
            'model_performance': self.results['evaluation_results'],
            'ensemble_configuration': {
                'models': list(self.ensemble_system.models.keys()),
                'weights': self.ensemble_system.weights,
                'feature_count': self.results['feature_count']
            },
            'data_statistics': {
                'shape': self.results['data_shape'],
                'features': self.results['feature_count']
            }
        }
        
        with open('enhanced_model_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("âœ… å¢å¼ºæ¨¡å‹æŠ¥å‘Šå·²ä¿å­˜åˆ° enhanced_model_report.json")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ å¢å¼ºæ¨¡å‹ç³»ç»Ÿå¯åŠ¨")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    enhanced_system = EnhancedModelSystem()
    
    # è¿è¡Œå¢å¼ºå»ºæ¨¡
    data_path = "data/extended/extended_all_data_3years_20250623_215123.csv"
    factor_path = "factors/bitcoin_factors_20250623_215658.csv"
    
    results = enhanced_system.run_enhanced_modeling(
        data_path=data_path,
        factor_path=factor_path
    )
    
    print("\nğŸ¯ å¢å¼ºå»ºæ¨¡å®Œæˆï¼ä¸»è¦æ”¹è¿›:")
    print(f"   - ç‰¹å¾æ•°é‡: {results['feature_count']}")
    print(f"   - æ•°æ®å½¢çŠ¶: {results['data_shape']}")
    print(f"   - é›†æˆæ¨¡å‹: {len(results['ensemble_weights'])} ä¸ª")
    
    # æ˜¾ç¤ºæ€§èƒ½æå‡
    if 'ensemble' in results['evaluation_results']:
        ensemble_perf = results['evaluation_results']['ensemble']
        print(f"   - é›†æˆRÂ²: {ensemble_perf['r2_mean']:.4f}")
        print(f"   - æ–¹å‘å‡†ç¡®ç‡: {ensemble_perf['direction_accuracy_mean']:.4f}")


if __name__ == "__main__":
    main() 