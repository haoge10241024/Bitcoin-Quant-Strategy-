#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ê†∏ÂøÉÊäÄÊúØÊîπËøõÁ≥ªÁªü
ÈáçÁÇπËß£ÂÜ≥ÔºöÊï∞ÊçÆË¥®ÈáèÈ™åËØÅ„ÄÅÂºÇÂ∏∏ÂÄºÊ£ÄÊµã„ÄÅOHLCÈÄªËæëÊ£ÄÊü•„ÄÅICÊµãËØï„ÄÅÂõ†Â≠êÊúâÊïàÊÄßÈ™åËØÅ„ÄÅÂÖ±Á∫øÊÄßÂ§ÑÁêÜ„ÄÅÂâçÁûªÂÅèÂ∑ÆÊ£ÄÊü•„ÄÅÁ≥ªÁªüÂåñË∂ÖÂèÇÊï∞‰ºòÂåñ
"""

import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
import json
import logging
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ÁªüËÆ°ÂíåÊú∫Âô®Â≠¶‰π†
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import randint, uniform
import matplotlib.pyplot as plt
import seaborn as sns

class DataQualityValidator:
    """Êï∞ÊçÆË¥®ÈáèÈ™åËØÅÂô®"""
    
    def __init__(self, missing_threshold=0.05, outlier_threshold=3.0):
        self.missing_threshold = missing_threshold
        self.outlier_threshold = outlier_threshold
        self.quality_issues = []
        
    def comprehensive_quality_check(self, df):
        """ÂÖ®Èù¢Êï∞ÊçÆË¥®ÈáèÊ£ÄÊü•"""
        print("üîç ÂºÄÂßãÊï∞ÊçÆË¥®ÈáèÊ£ÄÊü•...")
        self.quality_issues = []
        
        # 1. Áº∫Â§±ÂÄºÊ£ÄÊü•
        missing_issues = self._check_missing_values(df)
        self.quality_issues.extend(missing_issues)
        
        # 2. OHLCÈÄªËæëÊ£ÄÊü•
        ohlc_issues = self._check_ohlc_logic(df)
        self.quality_issues.extend(ohlc_issues)
        
        # 3. ÂºÇÂ∏∏ÂÄºÊ£ÄÊü•
        outlier_issues = self._check_outliers(df)
        self.quality_issues.extend(outlier_issues)
        
        # 4. Êó∂Èó¥Â∫èÂàóËøûÁª≠ÊÄßÊ£ÄÊü•
        time_issues = self._check_time_continuity(df)
        self.quality_issues.extend(time_issues)
        
        # 5. ‰ª∑Ê†ºË∑≥Ë∑ÉÊ£ÄÊü•
        jump_issues = self._check_price_jumps(df)
        self.quality_issues.extend(jump_issues)
        
        print(f"‚úÖ Êï∞ÊçÆË¥®ÈáèÊ£ÄÊü•ÂÆåÊàêÔºåÂèëÁé∞ {len(self.quality_issues)} ‰∏™ÈóÆÈ¢ò")
        return self.quality_issues
    
    def _check_missing_values(self, df):
        """Ê£ÄÊü•Áº∫Â§±ÂÄº"""
        issues = []
        price_cols = ['open', 'high', 'low', 'close', 'volume']
        
        for col in price_cols:
            if col in df.columns:
                missing_pct = df[col].isnull().sum() / len(df)
                if missing_pct > self.missing_threshold:
                    issues.append({
                        'type': 'missing_values',
                        'column': col,
                        'percentage': missing_pct,
                        'severity': 'high' if missing_pct > 0.1 else 'medium'
                    })
        
        return issues
    
    def _check_ohlc_logic(self, df):
        """Ê£ÄÊü•OHLCÈÄªËæë"""
        issues = []
        
        if all(col in df.columns for col in ['open', 'high', 'low', 'close']):
            # Ê£ÄÊü• high >= max(open, close)
            invalid_high = df['high'] < df[['open', 'close']].max(axis=1)
            if invalid_high.any():
                issues.append({
                    'type': 'ohlc_logic',
                    'problem': 'high_too_low',
                    'count': invalid_high.sum(),
                    'severity': 'high'
                })
            
            # Ê£ÄÊü• low <= min(open, close)
            invalid_low = df['low'] > df[['open', 'close']].min(axis=1)
            if invalid_low.any():
                issues.append({
                    'type': 'ohlc_logic',
                    'problem': 'low_too_high',
                    'count': invalid_low.sum(),
                    'severity': 'high'
                })
            
            # Ê£ÄÊü•ÈùûÊ≠£‰ª∑Ê†º
            for col in ['open', 'high', 'low', 'close']:
                non_positive = (df[col] <= 0).any()
                if non_positive:
                    issues.append({
                        'type': 'ohlc_logic',
                        'problem': 'non_positive_price',
                        'column': col,
                        'severity': 'critical'
                    })
        
        return issues
    
    def _check_outliers(self, df):
        """Ê£ÄÊü•ÂºÇÂ∏∏ÂÄº"""
        issues = []
        price_cols = ['open', 'high', 'low', 'close']
        
        for col in price_cols:
            if col in df.columns:
                # ËÆ°ÁÆóÊî∂ÁõäÁéá
                returns = df[col].pct_change().dropna()
                
                # ‰ΩøÁî®3œÉÂéüÂàôÊ£ÄÊµãÂºÇÂ∏∏ÂÄº
                mean_return = returns.mean()
                std_return = returns.std()
                
                outliers = np.abs(returns - mean_return) > self.outlier_threshold * std_return
                outlier_count = outliers.sum()
                
                if outlier_count > len(returns) * 0.01:  # Ë∂ÖËøá1%ÁöÑÊï∞ÊçÆÊòØÂºÇÂ∏∏ÂÄº
                    issues.append({
                        'type': 'outliers',
                        'column': col,
                        'count': outlier_count,
                        'percentage': outlier_count / len(returns),
                        'severity': 'medium'
                    })
        
        return issues
    
    def _check_time_continuity(self, df):
        """Ê£ÄÊü•Êó∂Èó¥Â∫èÂàóËøûÁª≠ÊÄß"""
        issues = []
        
        if 'datetime' in df.columns:
            df_sorted = df.sort_values('datetime')
            time_diffs = df_sorted['datetime'].diff().dropna()
            
            # Ê£ÄÊü•Êó∂Èó¥Èó¥ÈöîÁöÑ‰∏ÄËá¥ÊÄß
            mode_diff = time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else None
            
            if mode_diff:
                # ÊâæÂá∫Êó∂Èó¥Èó¥ÈöîÂºÇÂ∏∏ÁöÑËÆ∞ÂΩï
                abnormal_gaps = time_diffs != mode_diff
                if abnormal_gaps.any():
                    issues.append({
                        'type': 'time_continuity',
                        'problem': 'irregular_intervals',
                        'count': abnormal_gaps.sum(),
                        'severity': 'medium'
                    })
        
        return issues
    
    def _check_price_jumps(self, df):
        """Ê£ÄÊü•‰ª∑Ê†ºË∑≥Ë∑É"""
        issues = []
        
        if 'close' in df.columns:
            returns = df['close'].pct_change().dropna()
            
            # Ê£ÄÊü•ÊûÅÁ´Ø‰ª∑Ê†ºË∑≥Ë∑ÉÔºàË∂ÖËøá20%ÁöÑÂçïÊó•ÂèòÂåñÔºâ
            extreme_jumps = np.abs(returns) > 0.2
            jump_count = extreme_jumps.sum()
            
            if jump_count > 0:
                issues.append({
                    'type': 'price_jumps',
                    'count': jump_count,
                    'max_jump': np.abs(returns).max(),
                    'severity': 'medium'
                })
        
        return issues
    
    def auto_fix_data(self, df):
        """Ëá™Âä®‰øÆÂ§çÊï∞ÊçÆÈóÆÈ¢ò"""
        print("üîß ÂºÄÂßãËá™Âä®‰øÆÂ§çÊï∞ÊçÆÈóÆÈ¢ò...")
        fixed_df = df.copy()
        
        # 1. ‰øÆÂ§çOHLCÈÄªËæëÈîôËØØ
        fixed_df = self._fix_ohlc_logic(fixed_df)
        
        # 2. Â°´ÂÖÖÁº∫Â§±ÂÄº
        fixed_df = self._fill_missing_values(fixed_df)
        
        # 3. Âπ≥ÊªëÂºÇÂ∏∏ÂÄº
        fixed_df = self._smooth_outliers(fixed_df)
        
        print("‚úÖ Êï∞ÊçÆ‰øÆÂ§çÂÆåÊàê")
        return fixed_df
    
    def _fix_ohlc_logic(self, df):
        """‰øÆÂ§çOHLCÈÄªËæëÈîôËØØ"""
        if all(col in df.columns for col in ['open', 'high', 'low', 'close']):
            # ‰øÆÂ§çhighÂÄºÔºöÁ°Æ‰øùhigh >= max(open, close)
            df['high'] = np.maximum(df['high'], df[['open', 'close']].max(axis=1))
            
            # ‰øÆÂ§çlowÂÄºÔºöÁ°Æ‰øùlow <= min(open, close)
            df['low'] = np.minimum(df['low'], df[['open', 'close']].min(axis=1))
        
        return df
    
    def _fill_missing_values(self, df):
        """Â°´ÂÖÖÁº∫Â§±ÂÄº"""
        price_cols = ['open', 'high', 'low', 'close']
        
        for col in price_cols:
            if col in df.columns:
                # ‰ΩøÁî®ÂâçÂêëÂ°´ÂÖÖ
                df[col] = df[col].fillna(method='ffill')
                # Â¶ÇÊûúÂºÄÂ§¥ÊúâÁº∫Â§±ÂÄºÔºå‰ΩøÁî®ÂêéÂêëÂ°´ÂÖÖ
                df[col] = df[col].fillna(method='bfill')
        
        # Êàê‰∫§Èáè‰ΩøÁî®0Â°´ÂÖÖ
        if 'volume' in df.columns:
            df['volume'] = df['volume'].fillna(0)
        
        return df
    
    def _smooth_outliers(self, df):
        """Âπ≥ÊªëÂºÇÂ∏∏ÂÄº"""
        price_cols = ['open', 'high', 'low', 'close']
        
        for col in price_cols:
            if col in df.columns:
                # ‰ΩøÁî®ÂàÜ‰ΩçÊï∞ÊñπÊ≥ïÂ§ÑÁêÜÂºÇÂ∏∏ÂÄº
                Q1 = df[col].quantile(0.01)
                Q99 = df[col].quantile(0.99)
                
                df[col] = df[col].clip(lower=Q1, upper=Q99)
        
        return df


class FactorEffectivenessValidator:
    """Âõ†Â≠êÊúâÊïàÊÄßÈ™åËØÅÂô®"""
    
    def __init__(self, min_ic=0.02, min_ir=0.5, ic_window=20):
        self.min_ic = min_ic
        self.min_ir = min_ir
        self.ic_window = ic_window
        
    def comprehensive_factor_analysis(self, factors, target_returns):
        """ÂÖ®Èù¢Âõ†Â≠êÂàÜÊûê"""
        print("üìä ÂºÄÂßãÂõ†Â≠êÊúâÊïàÊÄßÂàÜÊûê...")
        
        results = {
            'ic_analysis': self._ic_analysis(factors, target_returns),
            'correlation_analysis': self._correlation_analysis(factors),
            'factor_selection': None,
            'multicollinearity_check': None
        }
        
        # Âõ†Â≠êÁ≠õÈÄâ
        effective_factors = self._select_effective_factors(
            factors, target_returns, results['ic_analysis']
        )
        results['factor_selection'] = effective_factors
        
        # ÂÖ±Á∫øÊÄßÊ£ÄÊü•
        if len(effective_factors['selected_factors']) > 1:
            multicollinearity = self._check_multicollinearity(
                factors[effective_factors['selected_factors']]
            )
            results['multicollinearity_check'] = multicollinearity
        
        print(f"‚úÖ Âõ†Â≠êÂàÜÊûêÂÆåÊàêÔºå‰ªé {len(factors.columns)} ‰∏™Âõ†Â≠ê‰∏≠Á≠õÈÄâÂá∫ {len(effective_factors['selected_factors'])} ‰∏™ÊúâÊïàÂõ†Â≠ê")
        
        return results
    
    def _ic_analysis(self, factors, target_returns):
        """ICÂàÜÊûêÔºà‰ø°ÊÅØÁ≥ªÊï∞ÂàÜÊûêÔºâ"""
        # Âè™ÈÄâÊã©Êï∞ÂÄºÂàóËøõË°åÂàÜÊûê
        numeric_factors = factors.select_dtypes(include=[np.number])
        
        ic_results = {}
        
        for factor_name in numeric_factors.columns:
            if factor_name in target_returns.index:
                continue
                
            ic_series = []
            
            # ÊªöÂä®ËÆ°ÁÆóIC
            for i in range(self.ic_window, len(numeric_factors)):
                try:
                    factor_window = numeric_factors[factor_name].iloc[i-self.ic_window:i]
                    return_window = target_returns.iloc[i-self.ic_window:i]
                    
                    # ÁßªÈô§Áº∫Â§±ÂÄº
                    valid_mask = ~(factor_window.isnull() | return_window.isnull())
                    if valid_mask.sum() < 10:  # Ëá≥Â∞ëÈúÄË¶Å10‰∏™ÊúâÊïàËßÇÊµã
                        ic_series.append(np.nan)
                        continue
                    
                    factor_clean = factor_window[valid_mask]
                    return_clean = return_window[valid_mask]
                    
                    # ËÆ°ÁÆóSpearmanÁõ∏ÂÖ≥Á≥ªÊï∞ÔºàÊõ¥Á®≥ÂÅ•Ôºâ
                    ic, _ = stats.spearmanr(factor_clean, return_clean)
                    ic_series.append(ic)
                    
                except Exception as e:
                    ic_series.append(np.nan)
            
            ic_series = pd.Series(ic_series).dropna()
            
            if len(ic_series) > 0:
                ic_results[factor_name] = {
                    'ic_mean': ic_series.mean(),
                    'ic_std': ic_series.std(),
                    'ic_ir': ic_series.mean() / ic_series.std() if ic_series.std() > 0 else 0,
                    'ic_positive_rate': (ic_series > 0).mean(),
                    'ic_t_stat': ic_series.mean() / (ic_series.std() / np.sqrt(len(ic_series))) if ic_series.std() > 0 else 0,
                    'is_effective': (abs(ic_series.mean()) > self.min_ic and 
                                   abs(ic_series.mean() / ic_series.std()) > self.min_ir)
                }
            else:
                ic_results[factor_name] = {
                    'ic_mean': 0,
                    'ic_std': 0,
                    'ic_ir': 0,
                    'ic_positive_rate': 0.5,
                    'ic_t_stat': 0,
                    'is_effective': False
                }
        
        return ic_results
    
    def _correlation_analysis(self, factors):
        """Âõ†Â≠êÁõ∏ÂÖ≥ÊÄßÂàÜÊûê"""
        # Âè™ÈÄâÊã©Êï∞ÂÄºÂàóËøõË°åÁõ∏ÂÖ≥ÊÄßÂàÜÊûê
        numeric_factors = factors.select_dtypes(include=[np.number])
        
        if len(numeric_factors.columns) == 0:
            return {
                'correlation_matrix': pd.DataFrame(),
                'high_correlation_pairs': [],
                'max_correlation': 0
            }
        
        correlation_matrix = numeric_factors.corr()
        
        # ÊâæÂá∫È´òÁõ∏ÂÖ≥ÊÄßÁöÑÂõ†Â≠êÂØπ
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = abs(correlation_matrix.iloc[i, j])
                if corr_value > 0.8:  # È´òÁõ∏ÂÖ≥ÊÄßÈòàÂÄº
                    high_corr_pairs.append({
                        'factor1': correlation_matrix.columns[i],
                        'factor2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        return {
            'correlation_matrix': correlation_matrix,
            'high_correlation_pairs': high_corr_pairs,
            'max_correlation': correlation_matrix.abs().values[np.triu_indices_from(correlation_matrix.values, k=1)].max() if len(correlation_matrix) > 1 else 0
        }
    
    def _select_effective_factors(self, factors, target_returns, ic_analysis):
        """ÈÄâÊã©ÊúâÊïàÂõ†Â≠ê"""
        # Âü∫‰∫éICÂàÜÊûêÈÄâÊã©Âõ†Â≠ê
        effective_factors = [
            factor for factor, result in ic_analysis.items()
            if result['is_effective']
        ]
        
        # ÊåâIC_IRÊéíÂ∫è
        factor_scores = [(factor, ic_analysis[factor]['ic_ir']) 
                        for factor in effective_factors]
        factor_scores.sort(key=lambda x: abs(x[1]), reverse=True)
        
        selected_factors = [factor for factor, score in factor_scores]
        
        return {
            'selected_factors': selected_factors,
            'factor_scores': dict(factor_scores),
            'selection_summary': {
                'total_factors': len(factors.columns),
                'effective_factors': len(effective_factors),
                'selection_rate': len(effective_factors) / len(factors.columns)
            }
        }
    
    def _check_multicollinearity(self, factors, threshold=0.8):
        """Ê£ÄÊü•Â§öÈáçÂÖ±Á∫øÊÄß"""
        # Âè™ÈÄâÊã©Êï∞ÂÄºÂàó
        numeric_factors = factors.select_dtypes(include=[np.number])
        
        if len(numeric_factors.columns) <= 1:
            return {
                'removed_factors': [],
                'final_factors': list(numeric_factors.columns),
                'multicollinearity_resolved': False
            }
        
        correlation_matrix = numeric_factors.corr().abs()
        
        # ÊâæÂá∫ÈúÄË¶ÅÁßªÈô§ÁöÑÂõ†Â≠ê
        to_remove = set()
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                if correlation_matrix.iloc[i, j] > threshold:
                    # ÁßªÈô§Áõ∏ÂÖ≥ÊÄßËæÉÈ´òÁöÑÂõ†Â≠ê‰∏≠IC_IRËæÉ‰ΩéÁöÑÈÇ£‰∏™
                    factor1 = correlation_matrix.columns[i]
                    factor2 = correlation_matrix.columns[j]
                    
                    if factor1 not in to_remove and factor2 not in to_remove:
                        to_remove.add(factor2)  # ÁÆÄÂçïÁ≠ñÁï•ÔºöÁßªÈô§ÂêéÈù¢ÁöÑÂõ†Â≠ê
        
        final_factors = [f for f in numeric_factors.columns if f not in to_remove]
        
        return {
            'removed_factors': list(to_remove),
            'final_factors': final_factors,
            'multicollinearity_resolved': len(to_remove) > 0
        }


class WalkForwardValidator:
    """Ê≠•ËøõÂºèÈ™åËØÅÂô® - ÈÅøÂÖçÂâçÁûªÂÅèÂ∑Æ"""
    
    def __init__(self, train_window=252, test_window=21, min_train_size=100):
        self.train_window = train_window
        self.test_window = test_window
        self.min_train_size = min_train_size
        
    def validate_model_performance(self, X, y, model_configs):
        """È™åËØÅÊ®°ÂûãÊÄßËÉΩ - Êó†ÂâçÁûªÂÅèÂ∑Æ"""
        print("üîÑ ÂºÄÂßãÊ≠•ËøõÂºèÊ®°ÂûãÈ™åËØÅ...")
        
        results = {}
        
        for model_name, model_config in model_configs.items():
            print(f"   È™åËØÅÊ®°Âûã: {model_name}")
            
            model_class = model_config['class']
            model_params = model_config.get('params', {})
            
            validation_result = self._walk_forward_validation(
                X, y, model_class, model_params
            )
            
            results[model_name] = validation_result
        
        print("‚úÖ Ê≠•ËøõÂºèÈ™åËØÅÂÆåÊàê")
        return results
    
    def _walk_forward_validation(self, X, y, model_class, model_params):
        """Ê≠•ËøõÂºèÈ™åËØÅ"""
        predictions = []
        actual_values = []
        model_scores = []
        feature_importance_history = []
        
        # Á°Æ‰øùÊï∞ÊçÆÂØπÈΩê
        common_index = X.index.intersection(y.index)
        X_aligned = X.loc[common_index]
        y_aligned = y.loc[common_index]
        
        # Ê≠•ËøõÂºèÈ™åËØÅ
        for i in range(self.train_window, len(X_aligned) - self.test_window, self.test_window):
            try:
                # ËÆ≠ÁªÉÈõÜÔºöÂè™‰ΩøÁî®ÂéÜÂè≤Êï∞ÊçÆ
                train_start = max(0, i - self.train_window)
                train_end = i
                
                X_train = X_aligned.iloc[train_start:train_end]
                y_train = y_aligned.iloc[train_start:train_end]
                
                # ÊµãËØïÈõÜÔºöÊú™Êù•Êï∞ÊçÆ
                test_start = i
                test_end = min(i + self.test_window, len(X_aligned))
                
                X_test = X_aligned.iloc[test_start:test_end]
                y_test = y_aligned.iloc[test_start:test_end]
                
                # Ê£ÄÊü•Êï∞ÊçÆË¥®Èáè
                if len(X_train) < self.min_train_size or len(X_test) == 0:
                    continue
                
                # Â§ÑÁêÜÁº∫Â§±ÂÄº
                X_train_clean = X_train.fillna(X_train.median())
                X_test_clean = X_test.fillna(X_train.median())  # Áî®ËÆ≠ÁªÉÈõÜÁöÑ‰∏≠‰ΩçÊï∞Â°´ÂÖÖÊµãËØïÈõÜ
                
                # ËÆ≠ÁªÉÊ®°Âûã
                model = model_class(**model_params)
                model.fit(X_train_clean, y_train)
                
                # È¢ÑÊµã
                pred = model.predict(X_test_clean)
                
                # ËÆ∞ÂΩïÁªìÊûú
                predictions.extend(pred)
                actual_values.extend(y_test.values)
                
                # ËØÑ‰º∞Ê®°Âûã
                if len(y_test) > 1:
                    score = r2_score(y_test, pred)
                    model_scores.append(score)
                
                # ËÆ∞ÂΩïÁâπÂæÅÈáçË¶ÅÊÄßÔºàÂ¶ÇÊûúÊ®°ÂûãÊîØÊåÅÔºâ
                if hasattr(model, 'feature_importances_'):
                    importance = dict(zip(X_train_clean.columns, model.feature_importances_))
                    feature_importance_history.append(importance)
                
            except Exception as e:
                print(f"   Ë≠¶Âëä: È™åËØÅËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØ: {e}")
                continue
        
        # ËÆ°ÁÆóÊÄª‰ΩìÊÄßËÉΩÊåáÊ†á
        if len(predictions) > 0 and len(actual_values) > 0:
            predictions = np.array(predictions)
            actual_values = np.array(actual_values)
            
            mse = mean_squared_error(actual_values, predictions)
            r2 = r2_score(actual_values, predictions)
            
            # ËÆ°ÁÆóÊñπÂêëÂáÜÁ°ÆÁéá
            actual_direction = np.sign(actual_values)
            pred_direction = np.sign(predictions)
            direction_accuracy = np.mean(actual_direction == pred_direction)
            
            return {
                'predictions': predictions,
                'actual': actual_values,
                'mse': mse,
                'r2_score': r2,
                'direction_accuracy': direction_accuracy,
                'model_scores': model_scores,
                'mean_score': np.mean(model_scores) if model_scores else 0,
                'score_std': np.std(model_scores) if model_scores else 0,
                'feature_importance_history': feature_importance_history,
                'validation_periods': len(model_scores)
            }
        else:
            return {
                'predictions': np.array([]),
                'actual': np.array([]),
                'mse': np.inf,
                'r2_score': -np.inf,
                'direction_accuracy': 0.5,
                'model_scores': [],
                'mean_score': 0,
                'score_std': 0,
                'feature_importance_history': [],
                'validation_periods': 0
            }


class HyperparameterOptimizer:
    """Á≥ªÁªüÂåñË∂ÖÂèÇÊï∞‰ºòÂåñÂô®"""
    
    def __init__(self, cv_folds=5, n_iter=50, random_state=42):
        self.cv_folds = cv_folds
        self.n_iter = n_iter
        self.random_state = random_state
        
    def optimize_hyperparameters(self, X, y, model_configs):
        """Á≥ªÁªüÂåñË∂ÖÂèÇÊï∞‰ºòÂåñ"""
        print("‚öôÔ∏è ÂºÄÂßãË∂ÖÂèÇÊï∞‰ºòÂåñ...")
        
        optimized_models = {}
        
        for model_name, config in model_configs.items():
            print(f"   ‰ºòÂåñÊ®°Âûã: {model_name}")
            
            model_class = config['class']
            param_distributions = config.get('param_distributions', {})
            
            if param_distributions:
                best_model = self._optimize_single_model(
                    X, y, model_class, param_distributions
                )
                optimized_models[model_name] = best_model
            else:
                # Â¶ÇÊûúÊ≤°ÊúâÂèÇÊï∞ÂàÜÂ∏ÉÔºå‰ΩøÁî®ÈªòËÆ§ÂèÇÊï∞
                optimized_models[model_name] = {
                    'model': model_class(),
                    'best_params': {},
                    'best_score': 0
                }
        
        print("‚úÖ Ë∂ÖÂèÇÊï∞‰ºòÂåñÂÆåÊàê")
        return optimized_models
    
    def _optimize_single_model(self, X, y, model_class, param_distributions):
        """‰ºòÂåñÂçï‰∏™Ê®°Âûã"""
        try:
            # Â§ÑÁêÜÁº∫Â§±ÂÄº
            X_clean = X.fillna(X.median())
            
            # Êó∂Èó¥Â∫èÂàó‰∫§ÂèâÈ™åËØÅ
            tscv = TimeSeriesSplit(n_splits=self.cv_folds)
            
            # ÈöèÊú∫ÊêúÁ¥¢
            model = model_class()
            search = RandomizedSearchCV(
                model,
                param_distributions,
                n_iter=self.n_iter,
                cv=tscv,
                scoring='r2',
                random_state=self.random_state,
                n_jobs=-1
            )
            
            search.fit(X_clean, y)
            
            return {
                'model': search.best_estimator_,
                'best_params': search.best_params_,
                'best_score': search.best_score_,
                'cv_results': search.cv_results_
            }
            
        except Exception as e:
            print(f"   Ë≠¶Âëä: ‰ºòÂåñËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØ: {e}")
            return {
                'model': model_class(),
                'best_params': {},
                'best_score': 0,
                'cv_results': {}
            }


class CoreTechnicalImprovementSystem:
    """Ê†∏ÂøÉÊäÄÊúØÊîπËøõÁ≥ªÁªü"""
    
    def __init__(self):
        self.data_validator = DataQualityValidator()
        self.factor_validator = FactorEffectivenessValidator()
        self.walk_forward_validator = WalkForwardValidator()
        self.hyperparameter_optimizer = HyperparameterOptimizer()
        
        self.results = {}
        
    def run_comprehensive_improvements(self, data_path, factor_path=None):
        """ËøêË°åÂÖ®Èù¢ÊäÄÊúØÊîπËøõ"""
        print("üöÄ ÂºÄÂßãÊ†∏ÂøÉÊäÄÊúØÊîπËøõ...")
        
        # 1. Êï∞ÊçÆË¥®ÈáèÈ™åËØÅÂíå‰øÆÂ§ç
        print("\n" + "="*50)
        print("Á¨¨1Ê≠•: Êï∞ÊçÆË¥®ÈáèÈ™åËØÅÂíå‰øÆÂ§ç")
        print("="*50)
        
        data = pd.read_csv(data_path)
        if 'datetime' in data.columns:
            data['datetime'] = pd.to_datetime(data['datetime'])
        
        quality_issues = self.data_validator.comprehensive_quality_check(data)
        fixed_data = self.data_validator.auto_fix_data(data)
        
        self.results['data_quality'] = {
            'issues': quality_issues,
            'fixed_data_shape': fixed_data.shape
        }
        
        # 2. Âõ†Â≠êÊúâÊïàÊÄßÈ™åËØÅ
        print("\n" + "="*50)
        print("Á¨¨2Ê≠•: Âõ†Â≠êÊúâÊïàÊÄßÈ™åËØÅ")
        print("="*50)
        
        if factor_path:
            factors = pd.read_csv(factor_path)
        else:
            # Â¶ÇÊûúÊ≤°ÊúâÊèê‰æõÂõ†Â≠êÊñá‰ª∂Ôºå‰ªéÊï∞ÊçÆ‰∏≠ËÆ°ÁÆóÂü∫Á°ÄÂõ†Â≠ê
            factors = self._calculate_basic_factors(fixed_data)
        
        # ËÆ°ÁÆóÁõÆÊ†áÂèòÈáèÔºàÊú™Êù•Êî∂ÁõäÁéáÔºâ
        target_returns = fixed_data.groupby(['symbol', 'timeframe'])['close'].pct_change().shift(-1)
        target_returns = target_returns.dropna()
        
        factor_analysis = self.factor_validator.comprehensive_factor_analysis(
            factors, target_returns
        )
        
        self.results['factor_analysis'] = factor_analysis
        
        # 3. Ëé∑ÂèñÊúâÊïàÂõ†Â≠ê
        effective_factors = factors[factor_analysis['factor_selection']['selected_factors']]
        
        # 4. Â§ÑÁêÜÂ§öÈáçÂÖ±Á∫øÊÄß
        if factor_analysis['multicollinearity_check']:
            final_factors = effective_factors[
                factor_analysis['multicollinearity_check']['final_factors']
            ]
        else:
            final_factors = effective_factors
        
        # 5. Ê≠•ËøõÂºèÊ®°ÂûãÈ™åËØÅ
        print("\n" + "="*50)
        print("Á¨¨3Ê≠•: Ê≠•ËøõÂºèÊ®°ÂûãÈ™åËØÅÔºàÈÅøÂÖçÂâçÁûªÂÅèÂ∑ÆÔºâ")
        print("="*50)
        
        model_configs = self._get_model_configs()
        
        validation_results = self.walk_forward_validator.validate_model_performance(
            final_factors, target_returns, model_configs
        )
        
        self.results['validation_results'] = validation_results
        
        # 6. Ë∂ÖÂèÇÊï∞‰ºòÂåñ
        print("\n" + "="*50)
        print("Á¨¨4Ê≠•: Á≥ªÁªüÂåñË∂ÖÂèÇÊï∞‰ºòÂåñ")
        print("="*50)
        
        optimization_configs = self._get_optimization_configs()
        
        optimized_models = self.hyperparameter_optimizer.optimize_hyperparameters(
            final_factors, target_returns, optimization_configs
        )
        
        self.results['optimized_models'] = optimized_models
        
        # 7. ÁîüÊàêÊîπËøõÊä•Âëä
        self._generate_improvement_report()
        
        print("\nüéâ Ê†∏ÂøÉÊäÄÊúØÊîπËøõÂÆåÊàêÔºÅ")
        return self.results
    
    def _calculate_basic_factors(self, data):
        """ËÆ°ÁÆóÂü∫Á°ÄÂõ†Â≠ê"""
        print("   ËÆ°ÁÆóÂü∫Á°ÄÂõ†Â≠ê...")
        
        factors_list = []
        
        for (symbol, timeframe), group in data.groupby(['symbol', 'timeframe']):
            group = group.reset_index(drop=True).copy()
            
            # ‰ª∑Ê†ºÂõ†Â≠ê
            for period in [5, 10, 20]:
                group[f'return_{period}'] = group['close'].pct_change(period)
                group[f'ma_{period}'] = group['close'].rolling(period).mean()
                group[f'volatility_{period}'] = group['close'].pct_change().rolling(period).std()
            
            # ÊäÄÊúØÊåáÊ†á
            group['rsi_14'] = self._calculate_rsi(group['close'], 14)
            group['macd'] = group['close'].ewm(span=12).mean() - group['close'].ewm(span=26).mean()
            
            factors_list.append(group)
        
        all_factors = pd.concat(factors_list, ignore_index=True)
        
        # ÈÄâÊã©Âõ†Â≠êÂàó
        factor_columns = [col for col in all_factors.columns 
                         if col not in ['timestamp', 'datetime', 'symbol', 'timeframe', 
                                      'open', 'high', 'low', 'close', 'volume']]
        
        return all_factors[factor_columns].fillna(0)
    
    def _calculate_rsi(self, prices, period=14):
        """ËÆ°ÁÆóRSI"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0).rolling(period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _get_model_configs(self):
        """Ëé∑ÂèñÊ®°ÂûãÈÖçÁΩÆ"""
        return {
            'LinearRegression': {
                'class': LinearRegression,
                'params': {}
            },
            'Ridge': {
                'class': Ridge,
                'params': {'alpha': 1.0}
            },
            'RandomForest': {
                'class': RandomForestRegressor,
                'params': {'n_estimators': 100, 'random_state': 42}
            },
            'GradientBoosting': {
                'class': GradientBoostingRegressor,
                'params': {'n_estimators': 100, 'random_state': 42}
            }
        }
    
    def _get_optimization_configs(self):
        """Ëé∑Âèñ‰ºòÂåñÈÖçÁΩÆ"""
        return {
            'RandomForest': {
                'class': RandomForestRegressor,
                'param_distributions': {
                    'n_estimators': randint(50, 200),
                    'max_depth': randint(3, 15),
                    'min_samples_split': randint(2, 10),
                    'min_samples_leaf': randint(1, 5)
                }
            },
            'GradientBoosting': {
                'class': GradientBoostingRegressor,
                'param_distributions': {
                    'n_estimators': randint(50, 200),
                    'learning_rate': uniform(0.01, 0.2),
                    'max_depth': randint(3, 8),
                    'subsample': uniform(0.8, 0.2)
                }
            }
        }
    
    def _generate_improvement_report(self):
        """ÁîüÊàêÊîπËøõÊä•Âëä"""
        print("\nüìä ÁîüÊàêÊîπËøõÊä•Âëä...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'improvements_summary': {
                'data_quality_issues_found': len(self.results['data_quality']['issues']),
                'factors_analyzed': len(self.results['factor_analysis']['ic_analysis']),
                'effective_factors_selected': len(self.results['factor_analysis']['factor_selection']['selected_factors']),
                'models_validated': len(self.results['validation_results']),
                'models_optimized': len(self.results['optimized_models'])
            },
            'best_model_performance': self._get_best_model_performance(),
            'factor_effectiveness_summary': self._get_factor_effectiveness_summary()
        }
        
        # ‰øùÂ≠òÊä•Âëä
        with open('core_technical_improvement_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("‚úÖ ÊîπËøõÊä•ÂëäÂ∑≤‰øùÂ≠òÂà∞ core_technical_improvement_report.json")
    
    def _get_best_model_performance(self):
        """Ëé∑ÂèñÊúÄ‰Ω≥Ê®°ÂûãÊÄßËÉΩ"""
        best_model = None
        best_score = -np.inf
        
        for model_name, results in self.results['validation_results'].items():
            if results['r2_score'] > best_score:
                best_score = results['r2_score']
                best_model = model_name
        
        return {
            'best_model': best_model,
            'best_r2_score': best_score,
            'best_direction_accuracy': self.results['validation_results'][best_model]['direction_accuracy'] if best_model else 0
        }
    
    def _get_factor_effectiveness_summary(self):
        """Ëé∑ÂèñÂõ†Â≠êÊúâÊïàÊÄßÊÄªÁªì"""
        ic_analysis = self.results['factor_analysis']['ic_analysis']
        
        effective_count = sum(1 for result in ic_analysis.values() if result['is_effective'])
        
        # ÊâæÂá∫ÊúÄ‰Ω≥Âõ†Â≠ê
        best_factors = sorted(
            [(factor, result['ic_ir']) for factor, result in ic_analysis.items()],
            key=lambda x: abs(x[1]),
            reverse=True
        )[:5]
        
        return {
            'total_factors': len(ic_analysis),
            'effective_factors': effective_count,
            'effectiveness_rate': effective_count / len(ic_analysis) if ic_analysis else 0,
            'top_5_factors': [factor for factor, score in best_factors]
        }


def main():
    """‰∏ªÂáΩÊï∞"""
    print("üîß Ê†∏ÂøÉÊäÄÊúØÊîπËøõÁ≥ªÁªüÂêØÂä®")
    
    # ÂàùÂßãÂåñÊîπËøõÁ≥ªÁªü
    improvement_system = CoreTechnicalImprovementSystem()
    
    # ËøêË°åÊîπËøõ
    data_path = "data/extended/extended_all_data_3years_20250623_215123.csv"
    factor_path = "factors/bitcoin_factors_20250623_215658.csv"
    
    results = improvement_system.run_comprehensive_improvements(
        data_path=data_path,
        factor_path=factor_path
    )
    
    print("\nüéØ ÊîπËøõÂÆåÊàêÔºÅ‰∏ªË¶ÅÊàêÊûú:")
    print(f"   - Êï∞ÊçÆË¥®ÈáèÈóÆÈ¢ò: {len(results['data_quality']['issues'])} ‰∏™")
    print(f"   - ÊúâÊïàÂõ†Â≠êÁ≠õÈÄâ: {len(results['factor_analysis']['factor_selection']['selected_factors'])} ‰∏™")
    print(f"   - Ê®°ÂûãÈ™åËØÅÂÆåÊàê: {len(results['validation_results'])} ‰∏™Ê®°Âûã")
    print(f"   - ÊúÄ‰Ω≥Ê®°Âûã: {results.get('best_model_performance', {}).get('best_model', 'N/A')}")


if __name__ == "__main__":
    main() 