#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¸å¿ƒæŠ€æœ¯æ”¹è¿›ç³»ç»Ÿ
é‡ç‚¹è§£å†³ï¼šæ•°æ®è´¨é‡éªŒè¯ã€å¼‚å¸¸å€¼æ£€æµ‹ã€OHLCé€»è¾‘æ£€æŸ¥ã€ICæµ‹è¯•ã€å› å­æœ‰æ•ˆæ€§éªŒè¯ã€å…±çº¿æ€§å¤„ç†ã€å‰ç»åå·®æ£€æŸ¥ã€ç³»ç»ŸåŒ–è¶…å‚æ•°ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
import json
import logging
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ 
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import randint, uniform
import matplotlib.pyplot as plt
import seaborn as sns

class DataQualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""
    
    def __init__(self, missing_threshold=0.05, outlier_threshold=3.0):
        self.missing_threshold = missing_threshold
        self.outlier_threshold = outlier_threshold
        self.quality_issues = []
        
    def comprehensive_quality_check(self, df):
        """å…¨é¢æ•°æ®è´¨é‡æ£€æŸ¥"""
        print("ğŸ” å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥...")
        self.quality_issues = []
        
        # 1. ç¼ºå¤±å€¼æ£€æŸ¥
        missing_issues = self._check_missing_values(df)
        self.quality_issues.extend(missing_issues)
        
        # 2. OHLCé€»è¾‘æ£€æŸ¥
        ohlc_issues = self._check_ohlc_logic(df)
        self.quality_issues.extend(ohlc_issues)
        
        # 3. å¼‚å¸¸å€¼æ£€æŸ¥
        outlier_issues = self._check_outliers(df)
        self.quality_issues.extend(outlier_issues)
        
        # 4. æ—¶é—´åºåˆ—è¿ç»­æ€§æ£€æŸ¥
        time_issues = self._check_time_continuity(df)
        self.quality_issues.extend(time_issues)
        
        # 5. ä»·æ ¼è·³è·ƒæ£€æŸ¥
        jump_issues = self._check_price_jumps(df)
        self.quality_issues.extend(jump_issues)
        
        print(f"âœ… æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆï¼Œå‘ç° {len(self.quality_issues)} ä¸ªé—®é¢˜")
        return self.quality_issues
    
    def _check_missing_values(self, df):
        """æ£€æŸ¥ç¼ºå¤±å€¼"""
        issues = []
        price_cols = ['open', 'high', 'low', 'close', 'volume']
        
        for col in price_cols:
            if col in df.columns:
                missing_pct = df[col].isnull().sum() / len(df)
                if missing_pct > self.missing_threshold:
                    issues.append({
                        'type': 'missing_values',
                        'column': col,
                        'percentage': missing_pct,
                        'severity': 'high' if missing_pct > 0.1 else 'medium'
                    })
        
        return issues
    
    def _check_ohlc_logic(self, df):
        """æ£€æŸ¥OHLCé€»è¾‘"""
        issues = []
        
        if all(col in df.columns for col in ['open', 'high', 'low', 'close']):
            # æ£€æŸ¥ high >= max(open, close)
            invalid_high = df['high'] < df[['open', 'close']].max(axis=1)
            if invalid_high.any():
                issues.append({
                    'type': 'ohlc_logic',
                    'problem': 'high_too_low',
                    'count': invalid_high.sum(),
                    'severity': 'high'
                })
            
            # æ£€æŸ¥ low <= min(open, close)
            invalid_low = df['low'] > df[['open', 'close']].min(axis=1)
            if invalid_low.any():
                issues.append({
                    'type': 'ohlc_logic',
                    'problem': 'low_too_high',
                    'count': invalid_low.sum(),
                    'severity': 'high'
                })
            
            # æ£€æŸ¥éæ­£ä»·æ ¼
            for col in ['open', 'high', 'low', 'close']:
                non_positive = (df[col] <= 0).any()
                if non_positive:
                    issues.append({
                        'type': 'ohlc_logic',
                        'problem': 'non_positive_price',
                        'column': col,
                        'severity': 'critical'
                    })
        
        return issues
    
    def _check_outliers(self, df):
        """æ£€æŸ¥å¼‚å¸¸å€¼"""
        issues = []
        price_cols = ['open', 'high', 'low', 'close']
        
        for col in price_cols:
            if col in df.columns:
                # è®¡ç®—æ”¶ç›Šç‡
                returns = df[col].pct_change().dropna()
                
                # ä½¿ç”¨3ÏƒåŸåˆ™æ£€æµ‹å¼‚å¸¸å€¼
                mean_return = returns.mean()
                std_return = returns.std()
                
                outliers = np.abs(returns - mean_return) > self.outlier_threshold * std_return
                outlier_count = outliers.sum()
                
                if outlier_count > len(returns) * 0.01:  # è¶…è¿‡1%çš„æ•°æ®æ˜¯å¼‚å¸¸å€¼
                    issues.append({
                        'type': 'outliers',
                        'column': col,
                        'count': outlier_count,
                        'percentage': outlier_count / len(returns),
                        'severity': 'medium'
                    })
        
        return issues
    
    def _check_time_continuity(self, df):
        """æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§"""
        issues = []
        
        if 'datetime' in df.columns:
            df_sorted = df.sort_values('datetime')
            time_diffs = df_sorted['datetime'].diff().dropna()
            
            # æ£€æŸ¥æ—¶é—´é—´éš”çš„ä¸€è‡´æ€§
            mode_diff = time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else None
            
            if mode_diff:
                # æ‰¾å‡ºæ—¶é—´é—´éš”å¼‚å¸¸çš„è®°å½•
                abnormal_gaps = time_diffs != mode_diff
                if abnormal_gaps.any():
                    issues.append({
                        'type': 'time_continuity',
                        'problem': 'irregular_intervals',
                        'count': abnormal_gaps.sum(),
                        'severity': 'medium'
                    })
        
        return issues
    
    def _check_price_jumps(self, df):
        """æ£€æŸ¥ä»·æ ¼è·³è·ƒ"""
        issues = []
        
        if 'close' in df.columns:
            returns = df['close'].pct_change().dropna()
            
            # æ£€æŸ¥æç«¯ä»·æ ¼è·³è·ƒï¼ˆè¶…è¿‡20%çš„å•æ—¥å˜åŒ–ï¼‰
            extreme_jumps = np.abs(returns) > 0.2
            jump_count = extreme_jumps.sum()
            
            if jump_count > 0:
                issues.append({
                    'type': 'price_jumps',
                    'count': jump_count,
                    'max_jump': np.abs(returns).max(),
                    'severity': 'medium'
                })
        
        return issues
    
    def auto_fix_data(self, df):
        """è‡ªåŠ¨ä¿®å¤æ•°æ®é—®é¢˜"""
        print("ğŸ”§ å¼€å§‹è‡ªåŠ¨ä¿®å¤æ•°æ®é—®é¢˜...")
        fixed_df = df.copy()
        
        # 1. ä¿®å¤OHLCé€»è¾‘é”™è¯¯
        fixed_df = self._fix_ohlc_logic(fixed_df)
        
        # 2. å¡«å……ç¼ºå¤±å€¼
        fixed_df = self._fill_missing_values(fixed_df)
        
        # 3. å¹³æ»‘å¼‚å¸¸å€¼
        fixed_df = self._smooth_outliers(fixed_df)
        
        print("âœ… æ•°æ®ä¿®å¤å®Œæˆ")
        return fixed_df
    
    def _fix_ohlc_logic(self, df):
        """ä¿®å¤OHLCé€»è¾‘é”™è¯¯"""
        if all(col in df.columns for col in ['open', 'high', 'low', 'close']):
            # ä¿®å¤highå€¼ï¼šç¡®ä¿high >= max(open, close)
            df['high'] = np.maximum(df['high'], df[['open', 'close']].max(axis=1))
            
            # ä¿®å¤lowå€¼ï¼šç¡®ä¿low <= min(open, close)
            df['low'] = np.minimum(df['low'], df[['open', 'close']].min(axis=1))
        
        return df
    
    def _fill_missing_values(self, df):
        """å¡«å……ç¼ºå¤±å€¼"""
        price_cols = ['open', 'high', 'low', 'close']
        
        for col in price_cols:
            if col in df.columns:
                # ä½¿ç”¨å‰å‘å¡«å……
                df[col] = df[col].fillna(method='ffill')
                # å¦‚æœå¼€å¤´æœ‰ç¼ºå¤±å€¼ï¼Œä½¿ç”¨åå‘å¡«å……
                df[col] = df[col].fillna(method='bfill')
        
        # æˆäº¤é‡ä½¿ç”¨0å¡«å……
        if 'volume' in df.columns:
            df['volume'] = df['volume'].fillna(0)
        
        return df
    
    def _smooth_outliers(self, df):
        """å¹³æ»‘å¼‚å¸¸å€¼"""
        price_cols = ['open', 'high', 'low', 'close']
        
        for col in price_cols:
            if col in df.columns:
                # ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•å¤„ç†å¼‚å¸¸å€¼
                Q1 = df[col].quantile(0.01)
                Q99 = df[col].quantile(0.99)
                
                df[col] = df[col].clip(lower=Q1, upper=Q99)
        
        return df


class FactorEffectivenessValidator:
    """å› å­æœ‰æ•ˆæ€§éªŒè¯å™¨"""
    
    def __init__(self, min_ic=0.02, min_ir=0.5, ic_window=20):
        self.min_ic = min_ic
        self.min_ir = min_ir
        self.ic_window = ic_window
        
    def comprehensive_factor_analysis(self, factors, target_returns):
        """å…¨é¢å› å­åˆ†æ"""
        print("ğŸ“Š å¼€å§‹å› å­æœ‰æ•ˆæ€§åˆ†æ...")
        
        results = {
            'ic_analysis': self._ic_analysis(factors, target_returns),
            'correlation_analysis': self._correlation_analysis(factors),
            'factor_selection': None,
            'multicollinearity_check': None
        }
        
        # å› å­ç­›é€‰
        effective_factors = self._select_effective_factors(
            factors, target_returns, results['ic_analysis']
        )
        results['factor_selection'] = effective_factors
        
        # å…±çº¿æ€§æ£€æŸ¥
        if len(effective_factors['selected_factors']) > 1:
            multicollinearity = self._check_multicollinearity(
                factors[effective_factors['selected_factors']]
            )
            results['multicollinearity_check'] = multicollinearity
        
        print(f"âœ… å› å­åˆ†æå®Œæˆï¼Œä» {len(factors.columns)} ä¸ªå› å­ä¸­ç­›é€‰å‡º {len(effective_factors['selected_factors'])} ä¸ªæœ‰æ•ˆå› å­")
        
        return results
    
    def _ic_analysis(self, factors, target_returns):
        """ICåˆ†æï¼ˆä¿¡æ¯ç³»æ•°åˆ†æï¼‰"""
        # åªé€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œåˆ†æ
        numeric_factors = factors.select_dtypes(include=[np.number])
        
        ic_results = {}
        
        for factor_name in numeric_factors.columns:
            if factor_name in target_returns.index:
                continue
                
            ic_series = []
            
            # æ»šåŠ¨è®¡ç®—IC
            for i in range(self.ic_window, len(numeric_factors)):
                try:
                    factor_window = numeric_factors[factor_name].iloc[i-self.ic_window:i]
                    return_window = target_returns.iloc[i-self.ic_window:i]
                    
                    # ç§»é™¤ç¼ºå¤±å€¼
                    valid_mask = ~(factor_window.isnull() | return_window.isnull())
                    if valid_mask.sum() < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆè§‚æµ‹
                        ic_series.append(np.nan)
                        continue
                    
                    factor_clean = factor_window[valid_mask]
                    return_clean = return_window[valid_mask]
                    
                    # è®¡ç®—Spearmanç›¸å…³ç³»æ•°ï¼ˆæ›´ç¨³å¥ï¼‰
                    ic, _ = stats.spearmanr(factor_clean, return_clean)
                    ic_series.append(ic)
                    
                except Exception as e:
                    ic_series.append(np.nan)
            
            ic_series = pd.Series(ic_series).dropna()
            
            if len(ic_series) > 0:
                ic_results[factor_name] = {
                    'ic_mean': ic_series.mean(),
                    'ic_std': ic_series.std(),
                    'ic_ir': ic_series.mean() / ic_series.std() if ic_series.std() > 0 else 0,
                    'ic_positive_rate': (ic_series > 0).mean(),
                    'ic_t_stat': ic_series.mean() / (ic_series.std() / np.sqrt(len(ic_series))) if ic_series.std() > 0 else 0,
                    'is_effective': (abs(ic_series.mean()) > self.min_ic and 
                                   abs(ic_series.mean() / ic_series.std()) > self.min_ir)
                }
            else:
                ic_results[factor_name] = {
                    'ic_mean': 0,
                    'ic_std': 0,
                    'ic_ir': 0,
                    'ic_positive_rate': 0.5,
                    'ic_t_stat': 0,
                    'is_effective': False
                }
        
        return ic_results
    
    def _correlation_analysis(self, factors):
        """å› å­ç›¸å…³æ€§åˆ†æ"""
        # åªé€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œç›¸å…³æ€§åˆ†æ
        numeric_factors = factors.select_dtypes(include=[np.number])
        
        if len(numeric_factors.columns) == 0:
            return {
                'correlation_matrix': pd.DataFrame(),
                'high_correlation_pairs': [],
                'max_correlation': 0
            }
        
        correlation_matrix = numeric_factors.corr()
        
        # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„å› å­å¯¹
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = abs(correlation_matrix.iloc[i, j])
                if corr_value > 0.8:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                    high_corr_pairs.append({
                        'factor1': correlation_matrix.columns[i],
                        'factor2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        return {
            'correlation_matrix': correlation_matrix,
            'high_correlation_pairs': high_corr_pairs,
            'max_correlation': correlation_matrix.abs().values[np.triu_indices_from(correlation_matrix.values, k=1)].max() if len(correlation_matrix) > 1 else 0
        }
    
    def _select_effective_factors(self, factors, target_returns, ic_analysis):
        """é€‰æ‹©æœ‰æ•ˆå› å­"""
        # åŸºäºICåˆ†æé€‰æ‹©å› å­
        effective_factors = [
            factor for factor, result in ic_analysis.items()
            if result['is_effective']
        ]
        
        # æŒ‰IC_IRæ’åº
        factor_scores = [(factor, ic_analysis[factor]['ic_ir']) 
                        for factor in effective_factors]
        factor_scores.sort(key=lambda x: abs(x[1]), reverse=True)
        
        selected_factors = [factor for factor, score in factor_scores]
        
        return {
            'selected_factors': selected_factors,
            'factor_scores': dict(factor_scores),
            'selection_summary': {
                'total_factors': len(factors.columns),
                'effective_factors': len(effective_factors),
                'selection_rate': len(effective_factors) / len(factors.columns)
            }
        }
    
    def _check_multicollinearity(self, factors, threshold=0.8):
        """æ£€æŸ¥å¤šé‡å…±çº¿æ€§"""
        # åªé€‰æ‹©æ•°å€¼åˆ—
        numeric_factors = factors.select_dtypes(include=[np.number])
        
        if len(numeric_factors.columns) <= 1:
            return {
                'removed_factors': [],
                'final_factors': list(numeric_factors.columns),
                'multicollinearity_resolved': False
            }
        
        correlation_matrix = numeric_factors.corr().abs()
        
        # æ‰¾å‡ºéœ€è¦ç§»é™¤çš„å› å­
        to_remove = set()
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                if correlation_matrix.iloc[i, j] > threshold:
                    # ç§»é™¤ç›¸å…³æ€§è¾ƒé«˜çš„å› å­ä¸­IC_IRè¾ƒä½çš„é‚£ä¸ª
                    factor1 = correlation_matrix.columns[i]
                    factor2 = correlation_matrix.columns[j]
                    
                    if factor1 not in to_remove and factor2 not in to_remove:
                        to_remove.add(factor2)  # ç®€å•ç­–ç•¥ï¼šç§»é™¤åé¢çš„å› å­
        
        final_factors = [f for f in numeric_factors.columns if f not in to_remove]
        
        return {
            'removed_factors': list(to_remove),
            'final_factors': final_factors,
            'multicollinearity_resolved': len(to_remove) > 0
        }


class WalkForwardValidator:
    """æ­¥è¿›å¼éªŒè¯å™¨ - é¿å…å‰ç»åå·®"""
    
    def __init__(self, train_window=252, test_window=21, min_train_size=100):
        self.train_window = train_window
        self.test_window = test_window
        self.min_train_size = min_train_size
        
    def validate_model_performance(self, X, y, model_configs):
        """éªŒè¯æ¨¡å‹æ€§èƒ½ - æ— å‰ç»åå·®"""
        print("ğŸ”„ å¼€å§‹æ­¥è¿›å¼æ¨¡å‹éªŒè¯...")
        
        results = {}
        
        for model_name, model_config in model_configs.items():
            print(f"   éªŒè¯æ¨¡å‹: {model_name}")
            
            model_class = model_config['class']
            model_params = model_config.get('params', {})
            
            validation_result = self._walk_forward_validation(
                X, y, model_class, model_params
            )
            
            results[model_name] = validation_result
        
        print("âœ… æ­¥è¿›å¼éªŒè¯å®Œæˆ")
        return results
    
    def _walk_forward_validation(self, X, y, model_class, model_params):
        """æ­¥è¿›å¼éªŒè¯"""
        predictions = []
        actual_values = []
        model_scores = []
        feature_importance_history = []
        
        # ç¡®ä¿æ•°æ®å¯¹é½
        common_index = X.index.intersection(y.index)
        X_aligned = X.loc[common_index]
        y_aligned = y.loc[common_index]
        
        # æ­¥è¿›å¼éªŒè¯
        for i in range(self.train_window, len(X_aligned) - self.test_window, self.test_window):
            try:
                # è®­ç»ƒé›†ï¼šåªä½¿ç”¨å†å²æ•°æ®
                train_start = max(0, i - self.train_window)
                train_end = i
                
                X_train = X_aligned.iloc[train_start:train_end]
                y_train = y_aligned.iloc[train_start:train_end]
                
                # æµ‹è¯•é›†ï¼šæœªæ¥æ•°æ®
                test_start = i
                test_end = min(i + self.test_window, len(X_aligned))
                
                X_test = X_aligned.iloc[test_start:test_end]
                y_test = y_aligned.iloc[test_start:test_end]
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                if len(X_train) < self.min_train_size or len(X_test) == 0:
                    continue
                
                # å¤„ç†ç¼ºå¤±å€¼
                X_train_clean = X_train.fillna(X_train.median())
                X_test_clean = X_test.fillna(X_train.median())  # ç”¨è®­ç»ƒé›†çš„ä¸­ä½æ•°å¡«å……æµ‹è¯•é›†
                
                # è®­ç»ƒæ¨¡å‹
                model = model_class(**model_params)
                model.fit(X_train_clean, y_train)
                
                # é¢„æµ‹
                pred = model.predict(X_test_clean)
                
                # è®°å½•ç»“æœ
                predictions.extend(pred)
                actual_values.extend(y_test.values)
                
                # è¯„ä¼°æ¨¡å‹
                if len(y_test) > 1:
                    score = r2_score(y_test, pred)
                    model_scores.append(score)
                
                # è®°å½•ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
                if hasattr(model, 'feature_importances_'):
                    importance = dict(zip(X_train_clean.columns, model.feature_importances_))
                    feature_importance_history.append(importance)
                
            except Exception as e:
                print(f"   è­¦å‘Š: éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                continue
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½æŒ‡æ ‡
        if len(predictions) > 0 and len(actual_values) > 0:
            predictions = np.array(predictions)
            actual_values = np.array(actual_values)
            
            mse = mean_squared_error(actual_values, predictions)
            r2 = r2_score(actual_values, predictions)
            
            # è®¡ç®—æ–¹å‘å‡†ç¡®ç‡
            actual_direction = np.sign(actual_values)
            pred_direction = np.sign(predictions)
            direction_accuracy = np.mean(actual_direction == pred_direction)
            
            return {
                'predictions': predictions,
                'actual': actual_values,
                'mse': mse,
                'r2_score': r2,
                'direction_accuracy': direction_accuracy,
                'model_scores': model_scores,
                'mean_score': np.mean(model_scores) if model_scores else 0,
                'score_std': np.std(model_scores) if model_scores else 0,
                'feature_importance_history': feature_importance_history,
                'validation_periods': len(model_scores)
            }
        else:
            return {
                'predictions': np.array([]),
                'actual': np.array([]),
                'mse': np.inf,
                'r2_score': -np.inf,
                'direction_accuracy': 0.5,
                'model_scores': [],
                'mean_score': 0,
                'score_std': 0,
                'feature_importance_history': [],
                'validation_periods': 0
            }


class HyperparameterOptimizer:
    """ç³»ç»ŸåŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, cv_folds=5, n_iter=50, random_state=42):
        self.cv_folds = cv_folds
        self.n_iter = n_iter
        self.random_state = random_state
        
    def optimize_hyperparameters(self, X, y, model_configs):
        """ç³»ç»ŸåŒ–è¶…å‚æ•°ä¼˜åŒ–"""
        print("âš™ï¸ å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")
        
        optimized_models = {}
        
        for model_name, config in model_configs.items():
            print(f"   ä¼˜åŒ–æ¨¡å‹: {model_name}")
            
            model_class = config['class']
            param_distributions = config.get('param_distributions', {})
            
            if param_distributions:
                best_model = self._optimize_single_model(
                    X, y, model_class, param_distributions
                )
                optimized_models[model_name] = best_model
            else:
                # å¦‚æœæ²¡æœ‰å‚æ•°åˆ†å¸ƒï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
                optimized_models[model_name] = {
                    'model': model_class(),
                    'best_params': {},
                    'best_score': 0
                }
        
        print("âœ… è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ")
        return optimized_models
    
    def _optimize_single_model(self, X, y, model_class, param_distributions):
        """ä¼˜åŒ–å•ä¸ªæ¨¡å‹"""
        try:
            # å¤„ç†ç¼ºå¤±å€¼
            X_clean = X.fillna(X.median())
            
            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            tscv = TimeSeriesSplit(n_splits=self.cv_folds)
            
            # éšæœºæœç´¢
            model = model_class()
            search = RandomizedSearchCV(
                model,
                param_distributions,
                n_iter=self.n_iter,
                cv=tscv,
                scoring='r2',
                random_state=self.random_state,
                n_jobs=-1
            )
            
            search.fit(X_clean, y)
            
            return {
                'model': search.best_estimator_,
                'best_params': search.best_params_,
                'best_score': search.best_score_,
                'cv_results': search.cv_results_
            }
            
        except Exception as e:
            print(f"   è­¦å‘Š: ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return {
                'model': model_class(),
                'best_params': {},
                'best_score': 0,
                'cv_results': {}
            }


class CoreTechnicalImprovementSystem:
    """æ ¸å¿ƒæŠ€æœ¯æ”¹è¿›ç³»ç»Ÿ"""
    
    def __init__(self):
        self.data_validator = DataQualityValidator()
        self.factor_validator = FactorEffectivenessValidator()
        self.walk_forward_validator = WalkForwardValidator()
        self.hyperparameter_optimizer = HyperparameterOptimizer()
        
        self.results = {}
        
    def run_comprehensive_improvements(self, data_path, factor_path=None):
        """è¿è¡Œå…¨é¢æŠ€æœ¯æ”¹è¿›"""
        print("ğŸš€ å¼€å§‹æ ¸å¿ƒæŠ€æœ¯æ”¹è¿›...")
        
        # 1. æ•°æ®è´¨é‡éªŒè¯å’Œä¿®å¤
        print("\n" + "="*50)
        print("ç¬¬1æ­¥: æ•°æ®è´¨é‡éªŒè¯å’Œä¿®å¤")
        print("="*50)
        
        data = pd.read_csv(data_path)
        if 'datetime' in data.columns:
            data['datetime'] = pd.to_datetime(data['datetime'])
        
        quality_issues = self.data_validator.comprehensive_quality_check(data)
        fixed_data = self.data_validator.auto_fix_data(data)
        
        self.results['data_quality'] = {
            'issues': quality_issues,
            'fixed_data_shape': fixed_data.shape
        }
        
        # 2. å› å­æœ‰æ•ˆæ€§éªŒè¯
        print("\n" + "="*50)
        print("ç¬¬2æ­¥: å› å­æœ‰æ•ˆæ€§éªŒè¯")
        print("="*50)
        
        if factor_path:
            factors = pd.read_csv(factor_path)
        else:
            # å¦‚æœæ²¡æœ‰æä¾›å› å­æ–‡ä»¶ï¼Œä»æ•°æ®ä¸­è®¡ç®—åŸºç¡€å› å­
            factors = self._calculate_basic_factors(fixed_data)
        
        # è®¡ç®—ç›®æ ‡å˜é‡ï¼ˆæœªæ¥æ”¶ç›Šç‡ï¼‰
        target_returns = fixed_data.groupby(['symbol', 'timeframe'])['close'].pct_change().shift(-1)
        target_returns = target_returns.dropna()
        
        factor_analysis = self.factor_validator.comprehensive_factor_analysis(
            factors, target_returns
        )
        
        self.results['factor_analysis'] = factor_analysis
        
        # 3. è·å–æœ‰æ•ˆå› å­
        effective_factors = factors[factor_analysis['factor_selection']['selected_factors']]
        
        # 4. å¤„ç†å¤šé‡å…±çº¿æ€§
        if factor_analysis['multicollinearity_check']:
            final_factors = effective_factors[
                factor_analysis['multicollinearity_check']['final_factors']
            ]
        else:
            final_factors = effective_factors
        
        # 5. æ­¥è¿›å¼æ¨¡å‹éªŒè¯
        print("\n" + "="*50)
        print("ç¬¬3æ­¥: æ­¥è¿›å¼æ¨¡å‹éªŒè¯ï¼ˆé¿å…å‰ç»åå·®ï¼‰")
        print("="*50)
        
        model_configs = self._get_model_configs()
        
        validation_results = self.walk_forward_validator.validate_model_performance(
            final_factors, target_returns, model_configs
        )
        
        self.results['validation_results'] = validation_results
        
        # 6. è¶…å‚æ•°ä¼˜åŒ–
        print("\n" + "="*50)
        print("ç¬¬4æ­¥: ç³»ç»ŸåŒ–è¶…å‚æ•°ä¼˜åŒ–")
        print("="*50)
        
        optimization_configs = self._get_optimization_configs()
        
        optimized_models = self.hyperparameter_optimizer.optimize_hyperparameters(
            final_factors, target_returns, optimization_configs
        )
        
        self.results['optimized_models'] = optimized_models
        
        # 7. ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š
        self._generate_improvement_report()
        
        print("\nğŸ‰ æ ¸å¿ƒæŠ€æœ¯æ”¹è¿›å®Œæˆï¼")
        return self.results
    
    def _calculate_basic_factors(self, data):
        """è®¡ç®—åŸºç¡€å› å­"""
        print("   è®¡ç®—åŸºç¡€å› å­...")
        
        factors_list = []
        
        for (symbol, timeframe), group in data.groupby(['symbol', 'timeframe']):
            group = group.reset_index(drop=True).copy()
            
            # ä»·æ ¼å› å­
            for period in [5, 10, 20]:
                group[f'return_{period}'] = group['close'].pct_change(period)
                group[f'ma_{period}'] = group['close'].rolling(period).mean()
                group[f'volatility_{period}'] = group['close'].pct_change().rolling(period).std()
            
            # æŠ€æœ¯æŒ‡æ ‡
            group['rsi_14'] = self._calculate_rsi(group['close'], 14)
            group['macd'] = group['close'].ewm(span=12).mean() - group['close'].ewm(span=26).mean()
            
            factors_list.append(group)
        
        all_factors = pd.concat(factors_list, ignore_index=True)
        
        # é€‰æ‹©å› å­åˆ—
        factor_columns = [col for col in all_factors.columns 
                         if col not in ['timestamp', 'datetime', 'symbol', 'timeframe', 
                                      'open', 'high', 'low', 'close', 'volume']]
        
        return all_factors[factor_columns].fillna(0)
    
    def _calculate_rsi(self, prices, period=14):
        """è®¡ç®—RSI"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0).rolling(period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _get_model_configs(self):
        """è·å–æ¨¡å‹é…ç½®"""
        return {
            'LinearRegression': {
                'class': LinearRegression,
                'params': {}
            },
            'Ridge': {
                'class': Ridge,
                'params': {'alpha': 1.0}
            },
            'RandomForest': {
                'class': RandomForestRegressor,
                'params': {'n_estimators': 100, 'random_state': 42}
            },
            'GradientBoosting': {
                'class': GradientBoostingRegressor,
                'params': {'n_estimators': 100, 'random_state': 42}
            }
        }
    
    def _get_optimization_configs(self):
        """è·å–ä¼˜åŒ–é…ç½®"""
        return {
            'RandomForest': {
                'class': RandomForestRegressor,
                'param_distributions': {
                    'n_estimators': randint(50, 200),
                    'max_depth': randint(3, 15),
                    'min_samples_split': randint(2, 10),
                    'min_samples_leaf': randint(1, 5)
                }
            },
            'GradientBoosting': {
                'class': GradientBoostingRegressor,
                'param_distributions': {
                    'n_estimators': randint(50, 200),
                    'learning_rate': uniform(0.01, 0.2),
                    'max_depth': randint(3, 8),
                    'subsample': uniform(0.8, 0.2)
                }
            }
        }
    
    def _generate_improvement_report(self):
        """ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ”¹è¿›æŠ¥å‘Š...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'improvements_summary': {
                'data_quality_issues_found': len(self.results['data_quality']['issues']),
                'factors_analyzed': len(self.results['factor_analysis']['ic_analysis']),
                'effective_factors_selected': len(self.results['factor_analysis']['factor_selection']['selected_factors']),
                'models_validated': len(self.results['validation_results']),
                'models_optimized': len(self.results['optimized_models'])
            },
            'best_model_performance': self._get_best_model_performance(),
            'factor_effectiveness_summary': self._get_factor_effectiveness_summary()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open('core_technical_improvement_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("âœ… æ”¹è¿›æŠ¥å‘Šå·²ä¿å­˜åˆ° core_technical_improvement_report.json")
    
    def _get_best_model_performance(self):
        """è·å–æœ€ä½³æ¨¡å‹æ€§èƒ½"""
        best_model = None
        best_score = -np.inf
        
        for model_name, results in self.results['validation_results'].items():
            if results['r2_score'] > best_score:
                best_score = results['r2_score']
                best_model = model_name
        
        return {
            'best_model': best_model,
            'best_r2_score': best_score,
            'best_direction_accuracy': self.results['validation_results'][best_model]['direction_accuracy'] if best_model else 0
        }
    
    def _get_factor_effectiveness_summary(self):
        """è·å–å› å­æœ‰æ•ˆæ€§æ€»ç»“"""
        ic_analysis = self.results['factor_analysis']['ic_analysis']
        
        effective_count = sum(1 for result in ic_analysis.values() if result['is_effective'])
        
        # æ‰¾å‡ºæœ€ä½³å› å­
        best_factors = sorted(
            [(factor, result['ic_ir']) for factor, result in ic_analysis.items()],
            key=lambda x: abs(x[1]),
            reverse=True
        )[:5]
        
        return {
            'total_factors': len(ic_analysis),
            'effective_factors': effective_count,
            'effectiveness_rate': effective_count / len(ic_analysis) if ic_analysis else 0,
            'top_5_factors': [factor for factor, score in best_factors]
        }


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ ¸å¿ƒæŠ€æœ¯æ”¹è¿›ç³»ç»Ÿå¯åŠ¨")
    
    # åˆå§‹åŒ–æ”¹è¿›ç³»ç»Ÿ
    improvement_system = CoreTechnicalImprovementSystem()
    
    # è¿è¡Œæ”¹è¿›
    data_path = "data/extended/extended_all_data_3years_20250623_215123.csv"
    factor_path = "factors/bitcoin_factors_20250623_215658.csv"
    
    results = improvement_system.run_comprehensive_improvements(
        data_path=data_path,
        factor_path=factor_path
    )
    
    print("\nğŸ¯ æ”¹è¿›å®Œæˆï¼ä¸»è¦æˆæœ:")
    print(f"   - æ•°æ®è´¨é‡é—®é¢˜: {len(results['data_quality']['issues'])} ä¸ª")
    print(f"   - æœ‰æ•ˆå› å­ç­›é€‰: {len(results['factor_analysis']['factor_selection']['selected_factors'])} ä¸ª")
    print(f"   - æ¨¡å‹éªŒè¯å®Œæˆ: {len(results['validation_results'])} ä¸ªæ¨¡å‹")
    print(f"   - æœ€ä½³æ¨¡å‹: {results.get('best_model_performance', {}).get('best_model', 'N/A')}")


if __name__ == "__main__":
    main() 