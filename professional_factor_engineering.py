#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸“ä¸šå› å­å·¥ç¨‹ç³»ç»Ÿ - åŒ…å«å®Œæ•´çš„é¢„å¤„ç†ã€ç­›é€‰å’Œåˆ†ææ–¹æ³•
"""

import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
import json
import logging
from datetime import datetime
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')

# ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ 
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class ProfessionalFactorEngineering:
    """ä¸“ä¸šå› å­å·¥ç¨‹ç³»ç»Ÿ"""
    
    def __init__(self, data_path: str = None):
        self.data_path = data_path or "data/extended/extended_all_data_3years_20250623_215123.csv"
        self.output_dir = Path("professional_factors")
        self.output_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–æ•°æ®
        self.raw_data = None
        self.factor_data = None
        self.processed_factors = None
        
        # å› å­åˆ†æç»“æœ
        self.factor_analysis = {}
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.output_dir / f"professional_factor_engineering_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        try:
            self.logger.info("åŠ è½½åŸå§‹æ•°æ®...")
            self.raw_data = pd.read_csv(self.data_path)
            self.raw_data['datetime'] = pd.to_datetime(self.raw_data['datetime'])
            
            self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(self.raw_data)} æ¡è®°å½•")
            return True
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def calculate_advanced_factors(self):
        """è®¡ç®—é«˜çº§å› å­"""
        try:
            self.logger.info("è®¡ç®—é«˜çº§å› å­...")
            
            processed_data = []
            
            for (symbol, timeframe), group in self.raw_data.groupby(['symbol', 'timeframe']):
                self.logger.info(f"å¤„ç† {symbol} {timeframe} æ•°æ®...")
                
                group = group.reset_index(drop=True).copy()
                
                # åŸºç¡€ä»·æ ¼å› å­
                group = self._calculate_price_factors(group)
                
                # é«˜çº§æŠ€æœ¯å› å­
                group = self._calculate_advanced_technical_factors(group)
                
                # ç»Ÿè®¡å› å­
                group = self._calculate_statistical_factors(group)
                
                # å¾®è§‚ç»“æ„å› å­
                group = self._calculate_microstructure_factors(group)
                
                # æ—¶é—´åºåˆ—å› å­
                group = self._calculate_time_series_factors(group)
                
                processed_data.append(group)
            
            self.factor_data = pd.concat(processed_data, ignore_index=True)
            self.logger.info(f"å› å­è®¡ç®—å®Œæˆï¼Œå…± {len(self.factor_data.columns)} åˆ—")
            
            return True
            
        except Exception as e:
            self.logger.error(f"å› å­è®¡ç®—å¤±è´¥: {e}")
            return False
    
    def _calculate_price_factors(self, df):
        """è®¡ç®—ä»·æ ¼ç›¸å…³å› å­"""
        # åŸºç¡€æ”¶ç›Šç‡
        for period in [1, 5, 10, 20]:
            df[f'return_{period}'] = df['close'].pct_change(period)
            df[f'log_return_{period}'] = np.log(df['close'] / df['close'].shift(period))
        
        # ä»·æ ¼ä½ç½®
        for period in [20, 60]:
            df[f'price_position_{period}'] = (df['close'] - df['close'].rolling(period).min()) / \
                                           (df['close'].rolling(period).max() - df['close'].rolling(period).min())
        
        # ç›¸å¯¹å¼ºå¼±
        for period in [10, 20]:
            df[f'relative_strength_{period}'] = df['close'] / df['close'].rolling(period).mean()
        
        return df
    
    def _calculate_advanced_technical_factors(self, df):
        """è®¡ç®—é«˜çº§æŠ€æœ¯å› å­"""
        # ç§»åŠ¨å¹³å‡ç³»ç»Ÿ
        for period in [5, 10, 20, 50]:
            df[f'ma_{period}'] = df['close'].rolling(period).mean()
            df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
            df[f'ma_ratio_{period}'] = df['close'] / df[f'ma_{period}']
        
        # MACDç³»ç»Ÿ
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']
        
        # RSIç³»ç»Ÿ
        for period in [6, 14, 24]:
            delta = df['close'].diff()
            gain = delta.where(delta > 0, 0).rolling(period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
            rs = gain / loss
            df[f'rsi_{period}'] = 100 - (100 / (1 + rs))
        
        # å¸ƒæ—å¸¦ç³»ç»Ÿ
        for period in [20]:
            ma = df['close'].rolling(period).mean()
            std = df['close'].rolling(period).std()
            df[f'bb_upper_{period}'] = ma + 2 * std
            df[f'bb_lower_{period}'] = ma - 2 * std
            df[f'bb_width_{period}'] = (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}']) / ma
            df[f'bb_position_{period}'] = (df['close'] - df[f'bb_lower_{period}']) / \
                                        (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}'])
        
        # ATRç³»ç»Ÿ
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        true_range = np.maximum(high_low, np.maximum(high_close, low_close))
        
        for period in [14, 21]:
            df[f'atr_{period}'] = true_range.rolling(period).mean()
            df[f'atr_ratio_{period}'] = df[f'atr_{period}'] / df['close']
        
        return df
    
    def _calculate_statistical_factors(self, df):
        """è®¡ç®—ç»Ÿè®¡å› å­"""
        # æ³¢åŠ¨ç‡ç³»ç»Ÿ
        for period in [5, 10, 20, 60]:
            returns = df['close'].pct_change()
            df[f'volatility_{period}'] = returns.rolling(period).std() * np.sqrt(252)
            df[f'volatility_ratio_{period}'] = df[f'volatility_{period}'] / df[f'volatility_{period}'].rolling(period).mean()
        
        # é«˜é˜¶çŸ©
        for period in [20, 60]:
            returns = df['close'].pct_change()
            df[f'skewness_{period}'] = returns.rolling(period).skew()
            df[f'kurtosis_{period}'] = returns.rolling(period).kurt()
        
        # é£é™©æŒ‡æ ‡
        for period in [20, 60]:
            returns = df['close'].pct_change()
            df[f'downside_deviation_{period}'] = returns[returns < 0].rolling(period).std()
            df[f'upside_deviation_{period}'] = returns[returns > 0].rolling(period).std()
            df[f'var_95_{period}'] = returns.rolling(period).quantile(0.05)
            df[f'cvar_95_{period}'] = returns[returns <= returns.rolling(period).quantile(0.05)].rolling(period).mean()
        
        return df
    
    def _calculate_microstructure_factors(self, df):
        """è®¡ç®—å¾®è§‚ç»“æ„å› å­"""
        # æˆäº¤é‡å› å­
        for period in [5, 10, 20]:
            df[f'volume_ma_{period}'] = df['volume'].rolling(period).mean()
            df[f'volume_ratio_{period}'] = df['volume'] / df[f'volume_ma_{period}']
            df[f'volume_std_{period}'] = df['volume'].rolling(period).std()
        
        # ä»·é‡å…³ç³»
        for period in [10, 20]:
            df[f'price_volume_corr_{period}'] = df['close'].rolling(period).corr(df['volume'])
            df[f'return_volume_corr_{period}'] = df['close'].pct_change().rolling(period).corr(df['volume'])
        
        # èµ„é‡‘æµå‘
        typical_price = (df['high'] + df['low'] + df['close']) / 3
        money_flow = typical_price * df['volume']
        positive_flow = money_flow.where(typical_price > typical_price.shift(), 0)
        negative_flow = money_flow.where(typical_price < typical_price.shift(), 0)
        
        for period in [14]:
            df[f'mfi_{period}'] = 100 - (100 / (1 + positive_flow.rolling(period).sum() / negative_flow.rolling(period).sum()))
        
        return df
    
    def _calculate_time_series_factors(self, df):
        """è®¡ç®—æ—¶é—´åºåˆ—å› å­"""
        # è¶‹åŠ¿å› å­
        for period in [5, 10, 20]:
            # çº¿æ€§å›å½’æ–œç‡
            def calc_slope(y):
                if len(y) < 2:
                    return np.nan
                x = np.arange(len(y))
                slope, _, _, _, _ = stats.linregress(x, y)
                return slope
            
            df[f'trend_slope_{period}'] = df['close'].rolling(period).apply(calc_slope, raw=False)
            df[f'trend_r2_{period}'] = df['close'].rolling(period).apply(
                lambda y: stats.linregress(np.arange(len(y)), y)[2]**2 if len(y) >= 2 else np.nan, 
                raw=False
            )
        
        # åŠ¨é‡å’Œåè½¬
        for period in [5, 10, 20]:
            df[f'momentum_{period}'] = df['close'].pct_change(period)
            df[f'reversal_{period}'] = -df[f'momentum_{period}']
            
            # é£é™©è°ƒæ•´åŠ¨é‡
            vol = df['close'].pct_change().rolling(period).std()
            df[f'momentum_vol_adj_{period}'] = df[f'momentum_{period}'] / vol
        
        return df
    
    def professional_preprocessing(self):
        """ä¸“ä¸šé¢„å¤„ç†"""
        try:
            self.logger.info("å¼€å§‹ä¸“ä¸šé¢„å¤„ç†...")
            
            # è·å–å› å­åˆ—
            factor_columns = [col for col in self.factor_data.columns 
                            if col not in ['timestamp', 'datetime', 'symbol', 'timeframe', 
                                         'open', 'high', 'low', 'close', 'volume']]
            
            processed_data = self.factor_data.copy()
            
            # 1. å¼‚å¸¸å€¼å¤„ç†ï¼ˆå»æå€¼ï¼‰
            self.logger.info("1. å¼‚å¸¸å€¼å¤„ç†...")
            for col in factor_columns:
                if processed_data[col].dtype in ['float64', 'int64']:
                    # ä½¿ç”¨3å€æ ‡å‡†å·®æ–¹æ³•
                    mean_val = processed_data[col].mean()
                    std_val = processed_data[col].std()
                    
                    # å»æå€¼
                    processed_data[col] = processed_data[col].clip(
                        lower=mean_val - 3*std_val,
                        upper=mean_val + 3*std_val
                    )
            
            # 2. æ ‡å‡†åŒ–å¤„ç†
            self.logger.info("2. æ ‡å‡†åŒ–å¤„ç†...")
            scaler = RobustScaler()  # ä½¿ç”¨RobustScalerï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
            
            # æŒ‰symbolå’Œtimeframeåˆ†ç»„æ ‡å‡†åŒ–
            standardized_data = []
            for (symbol, timeframe), group in processed_data.groupby(['symbol', 'timeframe']):
                group_copy = group.copy()
                
                # åªå¯¹å› å­åˆ—è¿›è¡Œæ ‡å‡†åŒ–
                factor_values = group_copy[factor_columns].values
                factor_values_clean = np.nan_to_num(factor_values, nan=0.0)
                
                if len(factor_values_clean) > 1:
                    factor_values_scaled = scaler.fit_transform(factor_values_clean)
                    group_copy[factor_columns] = factor_values_scaled
                
                standardized_data.append(group_copy)
            
            processed_data = pd.concat(standardized_data, ignore_index=True)
            
            # 3. ç¼ºå¤±å€¼å¤„ç†
            self.logger.info("3. ç¼ºå¤±å€¼å¤„ç†...")
            # ä½¿ç”¨å‰å‘å¡«å……å’Œåå‘å¡«å……
            processed_data[factor_columns] = processed_data[factor_columns].fillna(method='ffill').fillna(method='bfill')
            
            # 4. ä¸­æ€§åŒ–å¤„ç†ï¼ˆè¡Œä¸šä¸­æ€§åŒ–ï¼Œè¿™é‡Œç”¨symbolä»£æ›¿ï¼‰
            self.logger.info("4. ä¸­æ€§åŒ–å¤„ç†...")
            for col in factor_columns:
                if processed_data[col].dtype in ['float64', 'int64']:
                    # æŒ‰symbolè¿›è¡Œä¸­æ€§åŒ–
                    processed_data[f'{col}_neutral'] = processed_data.groupby('symbol')[col].transform(
                        lambda x: x - x.mean()
                    )
            
            self.processed_factors = processed_data
            self.logger.info("ä¸“ä¸šé¢„å¤„ç†å®Œæˆ")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸“ä¸šé¢„å¤„ç†å¤±è´¥: {e}")
            return False
    
    def professional_factor_selection(self, target_col: str = 'return_1', n_features: int = 20):
        """ä¸“ä¸šå› å­ç­›é€‰"""
        try:
            self.logger.info("å¼€å§‹ä¸“ä¸šå› å­ç­›é€‰...")
            
            # å‡†å¤‡æ•°æ®
            factor_columns = [col for col in self.processed_factors.columns 
                            if col not in ['timestamp', 'datetime', 'symbol', 'timeframe', 
                                         'open', 'high', 'low', 'close', 'volume']]
            
            # åˆ›å»ºç›®æ ‡å˜é‡
            self.processed_factors['target'] = self.processed_factors.groupby(['symbol', 'timeframe'])['close'].transform(
                lambda x: x.pct_change().shift(-1)
            )
            
            # ç§»é™¤ç¼ºå¤±å€¼
            clean_data = self.processed_factors.dropna(subset=['target'] + factor_columns[:50])  # é™åˆ¶å› å­æ•°é‡é¿å…å†…å­˜é—®é¢˜
            
            X = clean_data[factor_columns[:50]]
            y = clean_data['target']
            
            selection_results = {}
            
            # 1. ç›¸å…³æ€§åˆ†æ
            self.logger.info("1. ç›¸å…³æ€§åˆ†æ...")
            correlations = X.corrwith(y).abs().sort_values(ascending=False)
            selection_results['correlation'] = correlations.head(n_features).to_dict()
            
            # 2. äº’ä¿¡æ¯åˆ†æ
            self.logger.info("2. äº’ä¿¡æ¯åˆ†æ...")
            mi_scores = mutual_info_regression(X.fillna(0), y)
            mi_df = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)
            selection_results['mutual_info'] = mi_df.head(n_features).to_dict()
            
            # 3. Fç»Ÿè®¡é‡åˆ†æ
            self.logger.info("3. Fç»Ÿè®¡é‡åˆ†æ...")
            f_selector = SelectKBest(score_func=f_regression, k=n_features)
            f_selector.fit(X.fillna(0), y)
            f_scores = pd.Series(f_selector.scores_, index=X.columns).sort_values(ascending=False)
            selection_results['f_statistic'] = f_scores.head(n_features).to_dict()
            
            # 4. éšæœºæ£®æ—é‡è¦æ€§
            self.logger.info("4. éšæœºæ£®æ—é‡è¦æ€§...")
            rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
            rf.fit(X.fillna(0), y)
            rf_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
            selection_results['random_forest'] = rf_importance.head(n_features).to_dict()
            
            # 5. ICåˆ†æï¼ˆä¿¡æ¯ç³»æ•°ï¼‰
            self.logger.info("5. ICåˆ†æ...")
            ic_results = {}
            for factor in factor_columns[:30]:  # é™åˆ¶å› å­æ•°é‡
                factor_data = clean_data[[factor, 'target', 'symbol', 'timeframe']].dropna()
                if len(factor_data) > 10:
                    ic = factor_data.groupby(['symbol', 'timeframe']).apply(
                        lambda x: x[factor].corr(x['target']) if len(x) > 2 else np.nan
                    ).mean()
                    ic_results[factor] = ic
            
            ic_series = pd.Series(ic_results).abs().sort_values(ascending=False)
            selection_results['ic_analysis'] = ic_series.head(n_features).to_dict()
            
            # ç»¼åˆè¯„åˆ†
            self.logger.info("6. ç»¼åˆè¯„åˆ†...")
            all_factors = set()
            for method_results in selection_results.values():
                all_factors.update(method_results.keys())
            
            composite_scores = {}
            for factor in all_factors:
                score = 0
                count = 0
                for method, results in selection_results.items():
                    if factor in results:
                        # æ ‡å‡†åŒ–åˆ†æ•°
                        max_score = max(results.values())
                        if max_score > 0:
                            score += results[factor] / max_score
                            count += 1
                
                if count > 0:
                    composite_scores[factor] = score / count
            
            composite_ranking = pd.Series(composite_scores).sort_values(ascending=False)
            selection_results['composite'] = composite_ranking.head(n_features).to_dict()
            
            # ä¿å­˜ç»“æœ
            self.factor_analysis['selection_results'] = selection_results
            
            with open(self.output_dir / "professional_factor_selection.json", 'w', encoding='utf-8') as f:
                json.dump(selection_results, f, ensure_ascii=False, indent=2)
            
            self.logger.info("ä¸“ä¸šå› å­ç­›é€‰å®Œæˆ")
            return selection_results
            
        except Exception as e:
            self.logger.error(f"ä¸“ä¸šå› å­ç­›é€‰å¤±è´¥: {e}")
            return None
    
    def generate_professional_analysis(self):
        """ç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Š"""
        try:
            self.logger.info("ç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Š...")
            
            # åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨
            self._create_factor_analysis_plots()
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = {
                "ä¸“ä¸šå› å­å·¥ç¨‹åˆ†ææŠ¥å‘Š": {
                    "ç”Ÿæˆæ—¶é—´": datetime.now().isoformat(),
                    "æ•°æ®æ¦‚å†µ": {
                        "åŸå§‹æ•°æ®": len(self.raw_data),
                        "å¤„ç†åæ•°æ®": len(self.processed_factors),
                        "å› å­æ€»æ•°": len([col for col in self.processed_factors.columns 
                                      if col not in ['timestamp', 'datetime', 'symbol', 'timeframe', 
                                                   'open', 'high', 'low', 'close', 'volume', 'target']]),
                        "äº¤æ˜“å¯¹": self.processed_factors['symbol'].unique().tolist(),
                        "æ—¶é—´æ¡†æ¶": self.processed_factors['timeframe'].unique().tolist()
                    },
                    "é¢„å¤„ç†ç»“æœ": {
                        "å¼‚å¸¸å€¼å¤„ç†": "3å€æ ‡å‡†å·®å»æå€¼",
                        "æ ‡å‡†åŒ–æ–¹æ³•": "RobustScaler",
                        "ç¼ºå¤±å€¼å¤„ç†": "å‰å‘åå‘å¡«å……",
                        "ä¸­æ€§åŒ–å¤„ç†": "æŒ‰symbolä¸­æ€§åŒ–"
                    },
                    "å› å­ç­›é€‰ç»“æœ": self.factor_analysis.get('selection_results', {})
                }
            }
            
            # ä¿å­˜æŠ¥å‘Š
            with open(self.output_dir / "professional_analysis_report.json", 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å¤„ç†åçš„å› å­æ•°æ®
            self.processed_factors.to_csv(self.output_dir / "professional_factors.csv", index=False)
            
            self.logger.info("ä¸“ä¸šåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return report
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def _create_factor_analysis_plots(self):
        """åˆ›å»ºå› å­åˆ†æå›¾è¡¨"""
        try:
            # 1. å› å­ç­›é€‰ç»“æœå¯¹æ¯”
            if 'selection_results' in self.factor_analysis:
                fig, axes = plt.subplots(2, 3, figsize=(20, 12))
                fig.suptitle('ğŸ” ä¸“ä¸šå› å­ç­›é€‰ç»“æœå¯¹æ¯”', fontsize=16, fontweight='bold')
                
                methods = ['correlation', 'mutual_info', 'f_statistic', 'random_forest', 'ic_analysis', 'composite']
                colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
                
                for i, method in enumerate(methods):
                    if method in self.factor_analysis['selection_results']:
                        ax = axes[i//3, i%3]
                        data = self.factor_analysis['selection_results'][method]
                        
                        factors = list(data.keys())[:10]  # å–å‰10ä¸ª
                        scores = [data[f] for f in factors]
                        
                        bars = ax.barh(range(len(factors)), scores, color=colors[i])
                        ax.set_yticks(range(len(factors)))
                        ax.set_yticklabels(factors, fontsize=8)
                        ax.set_title(f'{method.replace("_", " ").title()}', fontweight='bold')
                        ax.grid(True, alpha=0.3)
                        
                        # æ·»åŠ æ•°å€¼æ ‡ç­¾
                        for j, (bar, score) in enumerate(zip(bars, scores)):
                            ax.text(bar.get_width() + max(scores)*0.01, bar.get_y() + bar.get_height()/2, 
                                   f'{score:.3f}', va='center', fontsize=8)
                
                plt.tight_layout()
                plt.savefig(self.output_dir / 'factor_selection_comparison.png', dpi=300, bbox_inches='tight')
                plt.close()
            
            self.logger.info("å› å­åˆ†æå›¾è¡¨åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå› å­åˆ†æå›¾è¡¨å¤±è´¥: {e}")
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„ä¸“ä¸šåˆ†æ"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹ä¸“ä¸šå› å­å·¥ç¨‹åˆ†æ...")
            
            # 1. åŠ è½½æ•°æ®
            if not self.load_data():
                return False
            
            # 2. è®¡ç®—é«˜çº§å› å­
            if not self.calculate_advanced_factors():
                return False
            
            # 3. ä¸“ä¸šé¢„å¤„ç†
            if not self.professional_preprocessing():
                return False
            
            # 4. ä¸“ä¸šå› å­ç­›é€‰
            selection_results = self.professional_factor_selection()
            if not selection_results:
                return False
            
            # 5. ç”Ÿæˆä¸“ä¸šåˆ†ææŠ¥å‘Š
            report = self.generate_professional_analysis()
            if not report:
                return False
            
            self.logger.info("ğŸ‰ ä¸“ä¸šå› å­å·¥ç¨‹åˆ†æå®Œæˆï¼")
            self.logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸“ä¸šåˆ†æå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸“ä¸šå› å­å·¥ç¨‹ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºä¸“ä¸šå› å­å·¥ç¨‹ç³»ç»Ÿ
    pfe = ProfessionalFactorEngineering()
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    success = pfe.run_complete_analysis()
    
    if success:
        print("\nâœ… ä¸“ä¸šå› å­å·¥ç¨‹åˆ†ææˆåŠŸå®Œæˆï¼")
        print("ğŸ“Š ä¸»è¦æ”¹è¿›:")
        print("  â€¢ ä¸“ä¸šçš„å¼‚å¸¸å€¼å¤„ç†ï¼ˆ3å€æ ‡å‡†å·®å»æå€¼ï¼‰")
        print("  â€¢ é²æ£’çš„æ ‡å‡†åŒ–å¤„ç†ï¼ˆRobustScalerï¼‰")
        print("  â€¢ å¤šç»´åº¦å› å­ç­›é€‰ï¼ˆç›¸å…³æ€§ã€äº’ä¿¡æ¯ã€Fç»Ÿè®¡é‡ã€éšæœºæ£®æ—ã€ICåˆ†æï¼‰")
        print("  â€¢ ä¸­æ€§åŒ–å¤„ç†å‡å°‘åå·®")
        print("  â€¢ ç»¼åˆè¯„åˆ†ç³»ç»Ÿ")
    else:
        print("\nâŒ ä¸“ä¸šå› å­å·¥ç¨‹åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

if __name__ == "__main__":
    main() 