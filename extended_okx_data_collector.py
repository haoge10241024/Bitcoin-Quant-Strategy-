#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰©å±•OKXæ•°æ®æ”¶é›†å™¨ - Extended OKX Data Collector
=============================================

è·å–æ›´é•¿æ—¶é—´è·¨åº¦çš„æ¯”ç‰¹å¸å†å²æ•°æ®ï¼ˆ2-3å¹´ï¼‰
ä¸ºé‡åŒ–åˆ†ææä¾›å……è¶³çš„æ•°æ®åŸºç¡€

Author: Professional Quantitative System
Version: 5.0 Extended Edition
"""

import ccxt
import pandas as pd
import numpy as np
from pathlib import Path
import json
import time
import logging
from datetime import datetime, timedelta
import sqlite3
from typing import Dict, List, Tuple, Optional
import os
import sys
from tqdm import tqdm

class ExtendedOKXDataCollector:
    """æ‰©å±•OKXæ•°æ®æ”¶é›†å™¨ - è·å–é•¿æœŸå†å²æ•°æ®"""
    
    def __init__(self, api_key: str = None, secret_key: str = None, passphrase: str = None):
        """
        åˆå§‹åŒ–æ‰©å±•æ•°æ®æ”¶é›†å™¨
        
        Args:
            api_key: OKX API Key
            secret_key: OKX Secret Key  
            passphrase: OKX API Passphrase
        """
        self.api_key = api_key
        self.secret_key = secret_key
        self.passphrase = passphrase
        self.okx = None
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
        (self.data_dir / "raw").mkdir(exist_ok=True)
        (self.data_dir / "processed").mkdir(exist_ok=True)
        (self.data_dir / "extended").mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('extended_okx_collector.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        self.initialize_okx()
    
    def initialize_okx(self):
        """åˆå§‹åŒ–OKXè¿æ¥"""
        try:
            # ä½¿ç”¨å…¬å…±APIæ¨¡å¼ï¼ˆæ›´ç¨³å®šï¼‰
            self.okx = ccxt.okx({
                'sandbox': False,
                'enableRateLimit': True,
                'timeout': 30000,
            })
            
            # æµ‹è¯•è¿æ¥
            markets = self.okx.load_markets()
            self.logger.info(f"OKXè¿æ¥æˆåŠŸï¼Œæ”¯æŒ {len(markets)} ä¸ªäº¤æ˜“å¯¹")
            
        except Exception as e:
            self.logger.error(f"OKXåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def get_extended_ohlcv_data(self, symbol: str = 'BTC/USDT', timeframe: str = '1d', 
                               start_date: str = None, end_date: str = None):
        """
        è·å–æ‰©å±•çš„OHLCVå†å²æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹
            timeframe: æ—¶é—´å‘¨æœŸ
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        """
        try:
            # è®¾ç½®é»˜è®¤æ—¶é—´èŒƒå›´ï¼ˆ3å¹´ï¼‰
            if not end_date:
                end_date = datetime.now()
            else:
                end_date = datetime.strptime(end_date, '%Y-%m-%d')
            
            if not start_date:
                start_date = end_date - timedelta(days=3*365)  # 3å¹´å‰
            else:
                start_date = datetime.strptime(start_date, '%Y-%m-%d')
            
            self.logger.info(f"è·å– {symbol} {timeframe} æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
            
            # è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_timestamp = int(start_date.timestamp() * 1000)
            end_timestamp = int(end_date.timestamp() * 1000)
            
            all_data = []
            current_timestamp = start_timestamp
            
            # æ ¹æ®æ—¶é—´å‘¨æœŸç¡®å®šæ¯æ¬¡è¯·æ±‚çš„æ•°é‡
            limit_map = {
                '1m': 1000,   # 1åˆ†é’Ÿ
                '5m': 1000,   # 5åˆ†é’Ÿ
                '15m': 1000,  # 15åˆ†é’Ÿ
                '1h': 1000,   # 1å°æ—¶
                '4h': 1000,   # 4å°æ—¶
                '1d': 1000,   # 1å¤©
                '1w': 1000,   # 1å‘¨
            }
            
            limit = limit_map.get(timeframe, 1000)
            
            # è®¡ç®—æ€»çš„é¢„æœŸè¯·æ±‚æ¬¡æ•°
            timeframe_minutes = {
                '1m': 1, '5m': 5, '15m': 15, '1h': 60, 
                '4h': 240, '1d': 1440, '1w': 10080
            }
            
            total_minutes = (end_timestamp - start_timestamp) / (1000 * 60)
            expected_bars = int(total_minutes / timeframe_minutes.get(timeframe, 1440))
            expected_requests = max(1, expected_bars // limit)
            
            self.logger.info(f"é¢„è®¡éœ€è¦ {expected_requests} æ¬¡è¯·æ±‚è·å–çº¦ {expected_bars} æ¡æ•°æ®")
            
            # ä½¿ç”¨è¿›åº¦æ¡
            with tqdm(total=expected_requests, desc=f"è·å–{symbol} {timeframe}æ•°æ®") as pbar:
                request_count = 0
                
                while current_timestamp < end_timestamp:
                    try:
                        # è·å–æ•°æ®
                        ohlcv = self.okx.fetch_ohlcv(
                            symbol, 
                            timeframe, 
                            since=current_timestamp, 
                            limit=limit
                        )
                        
                        if not ohlcv:
                            self.logger.warning(f"æ²¡æœ‰è·å–åˆ°æ•°æ®ï¼Œæ—¶é—´æˆ³: {current_timestamp}")
                            break
                        
                        # æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
                        all_data.extend(ohlcv)
                        
                        # æ›´æ–°æ—¶é—´æˆ³åˆ°æœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´
                        if ohlcv:
                            current_timestamp = ohlcv[-1][0] + 1
                        
                        request_count += 1
                        pbar.update(1)
                        pbar.set_postfix({
                            'Records': len(all_data),
                            'Latest': datetime.fromtimestamp(ohlcv[-1][0]/1000).strftime('%Y-%m-%d') if ohlcv else 'N/A'
                        })
                        
                        # é¿å…è¯·æ±‚è¿‡å¿«
                        time.sleep(0.1)
                        
                        # å¦‚æœè·å–çš„æ•°æ®å°‘äºlimitï¼Œè¯´æ˜å·²ç»åˆ°äº†æœ€æ–°æ•°æ®
                        if len(ohlcv) < limit:
                            break
                            
                    except Exception as e:
                        self.logger.error(f"è·å–æ•°æ®æ—¶å‡ºé”™: {e}")
                        time.sleep(1)  # å‡ºé”™æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                        continue
            
            if all_data:
                # å»é‡å¹¶æ’åº
                df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
                
                # æ·»åŠ å…¶ä»–å­—æ®µ
                df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
                df['symbol'] = symbol
                df['timeframe'] = timeframe
                
                self.logger.info(f"æˆåŠŸè·å– {len(df)} æ¡ {symbol} {timeframe} æ•°æ®")
                self.logger.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {df['datetime'].min()} åˆ° {df['datetime'].max()}")
                
                return df
            else:
                self.logger.warning(f"æœªè·å–åˆ°ä»»ä½• {symbol} {timeframe} æ•°æ®")
                return None
                
        except Exception as e:
            self.logger.error(f"è·å–æ‰©å±•æ•°æ®å¤±è´¥: {e}")
            return None
    
    def collect_multi_year_data(self, symbols: List[str] = None, timeframes: List[str] = None, 
                               years: int = 3):
        """
        æ”¶é›†å¤šå¹´å†å²æ•°æ®
        
        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            timeframes: æ—¶é—´å‘¨æœŸåˆ—è¡¨
            years: å†å²å¹´æ•°
        """
        if symbols is None:
            symbols = ['BTC/USDT', 'ETH/USDT']
        
        if timeframes is None:
            timeframes = ['1d', '4h', '1h']  # ä»å¤§åˆ°å°çš„æ—¶é—´å‘¨æœŸ
        
        self.logger.info(f"å¼€å§‹æ”¶é›† {years} å¹´å†å²æ•°æ®...")
        
        # è®¡ç®—å¼€å§‹æ—¥æœŸ
        end_date = datetime.now()
        start_date = end_date - timedelta(days=years*365)
        
        all_extended_data = {}
        
        for symbol in symbols:
            self.logger.info(f"å¤„ç†äº¤æ˜“å¯¹: {symbol}")
            all_extended_data[symbol] = {}
            
            for timeframe in timeframes:
                self.logger.info(f"è·å– {symbol} {timeframe} æ•°æ®...")
                
                df = self.get_extended_ohlcv_data(
                    symbol=symbol,
                    timeframe=timeframe,
                    start_date=start_date.strftime('%Y-%m-%d'),
                    end_date=end_date.strftime('%Y-%m-%d')
                )
                
                if df is not None and not df.empty:
                    all_extended_data[symbol][timeframe] = df
                    
                    # ä¿å­˜å•ä¸ªæ–‡ä»¶
                    filename = f"extended_{symbol.replace('/', '_')}_{timeframe}_{years}years.csv"
                    filepath = self.data_dir / "extended" / filename
                    df.to_csv(filepath, index=False)
                    self.logger.info(f"å·²ä¿å­˜: {filepath}")
                else:
                    self.logger.warning(f"æœªèƒ½è·å– {symbol} {timeframe} æ•°æ®")
                
                # è¯·æ±‚é—´éš”
                time.sleep(0.5)
        
        # ä¿å­˜ç»¼åˆæ•°æ®
        self.save_extended_data(all_extended_data, years)
        
        return all_extended_data
    
    def save_extended_data(self, data: Dict, years: int):
        """ä¿å­˜æ‰©å±•æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            all_dfs = []
            total_records = 0
            
            for symbol, timeframes in data.items():
                for timeframe, df in timeframes.items():
                    if df is not None and not df.empty:
                        all_dfs.append(df)
                        total_records += len(df)
            
            if all_dfs:
                # åˆå¹¶æ‰€æœ‰æ•°æ®
                combined_df = pd.concat(all_dfs, ignore_index=True)
                
                # ä¿å­˜åˆå¹¶çš„CSVæ–‡ä»¶
                combined_file = self.data_dir / "extended" / f"extended_all_data_{years}years_{timestamp}.csv"
                combined_df.to_csv(combined_file, index=False)
                self.logger.info(f"åˆå¹¶æ•°æ®å·²ä¿å­˜: {combined_file}")
                
                # ä¿å­˜åˆ°SQLiteæ•°æ®åº“
                db_file = self.data_dir / "processed" / f"extended_crypto_data_{years}years.db"
                with sqlite3.connect(db_file) as conn:
                    combined_df.to_sql('extended_ohlcv', conn, if_exists='replace', index=False)
                    self.logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“: {db_file}")
                
                # ç”Ÿæˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Š
                self.generate_extended_report(data, years, total_records, timestamp)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ‰©å±•æ•°æ®å¤±è´¥: {e}")
    
    def generate_extended_report(self, data: Dict, years: int, total_records: int, timestamp: str):
        """ç”Ÿæˆæ‰©å±•æ•°æ®æŠ¥å‘Š"""
        try:
            report = {
                "æ”¶é›†æ—¶é—´": datetime.now().isoformat(),
                "æ•°æ®æº": "OKX API - Extended Collection",
                "å†å²å¹´æ•°": years,
                "æ€»è®°å½•æ•°": total_records,
                "æ•°æ®ç»Ÿè®¡": {},
                "æ—¶é—´èŒƒå›´": {},
                "æ•°æ®è´¨é‡": {}
            }
            
            for symbol, timeframes in data.items():
                report["æ•°æ®ç»Ÿè®¡"][symbol] = {}
                report["æ—¶é—´èŒƒå›´"][symbol] = {}
                report["æ•°æ®è´¨é‡"][symbol] = {}
                
                for timeframe, df in timeframes.items():
                    if df is not None and not df.empty:
                        # åŸºç¡€ç»Ÿè®¡
                        report["æ•°æ®ç»Ÿè®¡"][symbol][timeframe] = {
                            "è®°å½•æ•°": len(df),
                            "å¼€å§‹æ—¶é—´": df['datetime'].min().isoformat(),
                            "ç»“æŸæ—¶é—´": df['datetime'].max().isoformat(),
                            "æ—¶é—´è·¨åº¦å¤©æ•°": (df['datetime'].max() - df['datetime'].min()).days
                        }
                        
                        # ä»·æ ¼ç»Ÿè®¡
                        report["æ—¶é—´èŒƒå›´"][symbol][timeframe] = {
                            "æœ€é«˜ä»·": f"${df['high'].max():,.2f}",
                            "æœ€ä½ä»·": f"${df['low'].min():,.2f}",
                            "æœ€æ–°ä»·": f"${df['close'].iloc[-1]:,.2f}",
                            "å¹³å‡ä»·": f"${df['close'].mean():,.2f}"
                        }
                        
                        # æ•°æ®è´¨é‡
                        missing_count = df.isnull().sum().sum()
                        report["æ•°æ®è´¨é‡"][symbol][timeframe] = {
                            "å®Œæ•´ç‡": f"{(1 - missing_count/len(df)) * 100:.2f}%",
                            "ç¼ºå¤±å€¼": int(missing_count),
                            "é‡å¤å€¼": int(df.duplicated().sum())
                        }
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = f"extended_data_report_{years}years_{timestamp}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æ‰©å±•æ•°æ®æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            # æ‰“å°æ‘˜è¦
            self.logger.info("=" * 60)
            self.logger.info(f"æ‰©å±•æ•°æ®æ”¶é›†å®Œæˆï¼({years}å¹´å†å²æ•°æ®)")
            self.logger.info(f"æ€»è®°å½•æ•°: {total_records:,} æ¡")
            for symbol in data.keys():
                symbol_total = sum(len(df) for df in data[symbol].values() if df is not None)
                self.logger.info(f"{symbol}: {symbol_total:,} æ¡è®°å½•")
            self.logger.info("=" * 60)
            
            return report
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ‰©å±•æŠ¥å‘Šå¤±è´¥: {e}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ‰©å±•OKXæ•°æ®æ”¶é›†å™¨ - è·å–é•¿æœŸå†å²æ•°æ®")
    print("=" * 60)
    
    # è¯¢é—®å†å²å¹´æ•°
    try:
        years_input = input("è¯·è¾“å…¥è¦æ”¶é›†çš„å†å²å¹´æ•° (å»ºè®®3-5å¹´ï¼Œé»˜è®¤3å¹´): ").strip()
        years = int(years_input) if years_input else 3
        years = max(1, min(years, 10))  # é™åˆ¶åœ¨1-10å¹´ä¹‹é—´
    except ValueError:
        years = 3
        print("è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼3å¹´")
    
    print(f"å°†æ”¶é›† {years} å¹´çš„å†å²æ•°æ®...")
    
    # åˆ›å»ºæ”¶é›†å™¨
    collector = ExtendedOKXDataCollector()
    
    try:
        # æ”¶é›†æ‰©å±•æ•°æ®
        print(f"\nğŸ“Š å¼€å§‹æ”¶é›† {years} å¹´å†å²æ•°æ®...")
        data = collector.collect_multi_year_data(
            symbols=['BTC/USDT', 'ETH/USDT'],
            timeframes=['1d', '4h', '1h'],  # ä»æ—¥çº¿å¼€å§‹ï¼Œé¿å…æ•°æ®é‡è¿‡å¤§
            years=years
        )
        
        print(f"\nğŸ‰ {years}å¹´å†å²æ•°æ®æ”¶é›†å®Œæˆï¼")
        print("ğŸ“ è¯·æŸ¥çœ‹ data/extended/ ç›®å½•ä¸‹çš„æ–‡ä»¶")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ”¶é›†å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… ç¨‹åºæ‰§è¡ŒæˆåŠŸï¼")
    else:
        print("\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼") 